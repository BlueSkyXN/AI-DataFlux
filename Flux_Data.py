# Flux_Data.py
import logging
import threading
import time
import os
import pandas as pd
from collections import defaultdict
from typing import Dict, Any, List, Tuple, Optional, Union
from abc import ABC, abstractmethod

# --- Conditional Import for MySQL ---
try:
    import mysql.connector
    from mysql.connector import pooling, errorcode
    MYSQL_AVAILABLE = True
    logging.debug("MySQL Connectoråº“å·²åŠ è½½ã€‚")
except ImportError:
    MYSQL_AVAILABLE = False
    logging.debug("MySQL Connectoråº“æœªåŠ è½½æˆ–ä¸å¯ç”¨ã€‚")
    # Define dummy error codes if needed elsewhere, though unlikely now
    class errorcode:
        ER_ACCESS_DENIED_ERROR = 1045
        ER_BAD_DB_ERROR = 1049
    class pooling: pass # Dummy class

# --- Conditional Import/Check for Pandas/Excel ---
try:
    # Check if pandas is installed
    pd.__version__
    # Check if openpyxl is installed (needed by pandas for xlsx)
    import openpyxl
    EXCEL_ENABLED = True
    logging.debug("Pandas å’Œ openpyxl åº“å·²åŠ è½½ã€‚")
except (ImportError, AttributeError):
    EXCEL_ENABLED = False
    logging.debug("Pandas æˆ– openpyxl åº“æœªåŠ è½½æˆ–ä¸å¯ç”¨ã€‚")


# --- Base Task Pool Abstract Base Class (Defined internally) ---
class BaseTaskPool(ABC):
    """Abstract base class defining the interface for data source task pools."""
    def __init__(self, columns_to_extract: List[str], columns_to_write: Dict[str, str], require_all_input_fields: bool = True):
        self.columns_to_extract = columns_to_extract
        self.columns_to_write = columns_to_write
        self.require_all_input_fields = require_all_input_fields  # æ˜¯å¦è¦æ±‚æ‰€æœ‰è¾“å…¥å­—æ®µéƒ½éç©º
        self.tasks: List[Tuple[Any, Dict[str, Any]]] = [] # In-memory list of tasks for the current shard: (task_id, data_dict)
        self.lock = threading.Lock() # Protects access to self.tasks list and potentially df in Excel pool

    @abstractmethod
    def get_total_task_count(self) -> int:
        """Return the total number of unprocessed tasks in the data source."""
        pass

    @abstractmethod
    def get_id_boundaries(self) -> Tuple[Any, Any]:
        """Return the minimum and maximum ID or index for sharding."""
        pass

    @abstractmethod
    def initialize_shard(self, shard_id: int, min_id: Any, max_id: Any) -> int:
        """Load unprocessed tasks for the given shard range into memory (self.tasks). Return number loaded."""
        pass

    @abstractmethod
    def get_task_batch(self, batch_size: int) -> List[Tuple[Any, Dict[str, Any]]]:
        """Get a batch of tasks from the in-memory list (self.tasks) and remove them."""
        pass

    @abstractmethod
    def update_task_results(self, results: Dict[Any, Dict[str, Any]]):
        """Write back the processing results for multiple tasks to the data source."""
        pass

    @abstractmethod
    def reload_task_data(self, task_id: Any) -> Optional[Dict[str, Any]]:
        """Reload the original input data for a specific task ID, used for retries. Return None if reload fails."""
        pass

    @abstractmethod
    def close(self):
        """Clean up resources (e.g., close connections, save files)."""
        pass

    # --- Concrete methods using the lock for self.tasks manipulation ---
    def add_task_to_front(self, task_id: Any, record_dict: Optional[Dict[str, Any]]):
        """Add a task back to the front of the in-memory queue (for retries)."""
        if record_dict is None:
             logging.warning(f"å°è¯•å°†ä»»åŠ¡ {task_id} æ”¾å›é˜Ÿåˆ—å¤±è´¥ï¼Œå› ä¸ºé‡è½½çš„æ•°æ®ä¸º Noneã€‚")
             return
        with self.lock:
            self.tasks.insert(0, (task_id, record_dict))
            logging.debug(f"ä»»åŠ¡ {task_id} å·²æ”¾å›é˜Ÿåˆ—å¤´éƒ¨ã€‚")

    def add_task_to_back(self, task_id: Any, record_dict: Optional[Dict[str, Any]]):
        """Add a task to the back of the in-memory queue (for deferred processing)."""
        if record_dict is None:
             logging.warning(f"å°è¯•å°†ä»»åŠ¡ {task_id} æ”¾å›é˜Ÿåˆ—å¤±è´¥ï¼Œå› ä¸ºæ•°æ®ä¸º Noneã€‚")
             return
        with self.lock:
            self.tasks.append((task_id, record_dict))
            logging.debug(f"ä»»åŠ¡ {task_id} å·²æ”¾å›é˜Ÿåˆ—å°¾éƒ¨ï¼Œç¨åå¤„ç†ã€‚")

    def has_tasks(self) -> bool:
        """Check if there are any tasks remaining in the current in-memory shard."""
        with self.lock:
            return len(self.tasks) > 0

    def get_remaining_count(self) -> int:
        """Get the number of tasks remaining in the current in-memory shard."""
        with self.lock:
            return len(self.tasks)

# --- MySQL Specific Components ---
if MYSQL_AVAILABLE:
    class MySQLConnectionPoolManager:
        """Manages a single MySQL connection pool instance."""
        _instance = None
        _lock = threading.Lock()
        _pool = None

        @classmethod
        def get_pool(cls, config: Optional[Dict[str, Any]] = None, pool_name="mypool", pool_size=5):
            if not MYSQL_AVAILABLE: # Redundant check but safe
                raise ImportError("MySQL Connectorä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºè¿æ¥æ± ã€‚")

            with cls._lock:
                if cls._instance is None:
                    if config is None:
                        raise ValueError("é¦–æ¬¡è·å–è¿æ¥æ± å¿…é¡»æä¾›æ•°æ®åº“é…ç½®ã€‚")
                    cls._instance = cls()
                    try:
                        logging.info(f"æ­£åœ¨åˆ›å»ºMySQLè¿æ¥æ±  '{pool_name}' (å¤§å°: {pool_size})...")
                        cls._pool = pooling.MySQLConnectionPool(
                            pool_name=pool_name,
                            pool_size=pool_size,
                            pool_reset_session=True,
                            host=config["host"],
                            port=config.get("port", 3306),
                            user=config["user"],
                            password=config["password"],
                            database=config["database"],
                            auth_plugin='mysql_native_password' # Or adjust as needed
                        )
                        logging.info(f"MySQLè¿æ¥æ±  '{pool_name}' åˆ›å»ºæˆåŠŸã€‚")
                    except mysql.connector.Error as err:
                        logging.error(f"åˆ›å»ºMySQLè¿æ¥æ± å¤±è´¥: {err}")
                        cls._instance = None # Reset instance if pool creation failed
                        raise # Re-raise the exception
                elif config is not None:
                    logging.warning("è¿æ¥æ± å·²å­˜åœ¨ï¼Œå°†å¿½ç•¥æ–°çš„é…ç½®å¹¶è¿”å›ç°æœ‰è¿æ¥æ± ã€‚")

                if cls._pool is None:
                    raise RuntimeError("è¿æ¥æ± å®ä¾‹å·²åˆ›å»ºä½†å†…éƒ¨æ± å¯¹è±¡ä¸ºNoneï¼Œå‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

                return cls._pool

        @classmethod
        def close_pool(cls):
            with cls._lock:
                if cls._instance is not None and cls._pool is not None:
                    logging.info(f"æ­£åœ¨å…³é—­MySQLè¿æ¥æ±  '{cls._pool.pool_name}'...")
                    cls._pool = None
                    cls._instance = None
                    logging.info("MySQLè¿æ¥æ± å®ä¾‹å·²æ¸…ç†ã€‚")
                elif cls._instance is not None:
                    logging.warning("å°è¯•å…³é—­è¿æ¥æ± ï¼Œä½†å†…éƒ¨æ± å¯¹è±¡å·²ä¸ºNoneã€‚")
                    cls._instance = None

    class MySQLTaskPool(BaseTaskPool):
        """MySQL data source task pool implementation."""
        def __init__(
            self,
            connection_config: Dict[str, Any],
            columns_to_extract: List[str],
            columns_to_write: Dict[str, str],
            table_name: str,
            pool_size: int = 5,
            require_all_input_fields: bool = True
        ):
            if not MYSQL_AVAILABLE:
                raise ImportError("MySQL Connectoråº“æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œæ— æ³•å®ä¾‹åŒ– MySQLTaskPoolã€‚")

            super().__init__(columns_to_extract, columns_to_write, require_all_input_fields) # Call super first

            self.table_name = table_name
            self.select_columns = list(set(['id'] + self.columns_to_extract))
            self.write_aliases = list(self.columns_to_write.keys())
            self.write_colnames = list(self.columns_to_write.values())

            try:
                self.pool = MySQLConnectionPoolManager.get_pool(
                    config=connection_config,
                    pool_size=pool_size
                )
            except Exception as e:
                logging.error(f"åˆå§‹åŒ– MySQLTaskPool æ—¶æ— æ³•è·å–æ•°æ®åº“è¿æ¥æ± : {e}")
                raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°æ•°æ®åº“æˆ–åˆ›å»ºè¿æ¥æ± : {e}") from e

            self.current_shard_id = -1
            self.current_min_id = 0
            self.current_max_id = 0
            logging.info(f"MySQLTaskPool åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡è¡¨: {self.table_name}")

        def _get_connection(self):
            """Gets a connection from the pool."""
            if not self.pool:
                raise RuntimeError("æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–ã€‚")
            try:
                conn = self.pool.get_connection()
                return conn
            except mysql.connector.Error as err:
                logging.error(f"ä»è¿æ¥æ±  '{self.pool.pool_name}' è·å–è¿æ¥å¤±è´¥: {err}")
                raise ConnectionError(f"æ— æ³•ä»æ•°æ®åº“è¿æ¥æ± è·å–è¿æ¥: {err}") from err

        def execute_with_connection(self, callback, is_write=False):
            """Executes a callback with a connection from the pool."""
            conn = None
            cursor = None
            try:
                conn = self._get_connection()
                # Use dictionary=True for easy access by column name
                cursor = conn.cursor(dictionary=True)
                result = callback(conn, cursor)
                if is_write:
                    conn.commit()
                return result
            except mysql.connector.Error as err:
                logging.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {err}")
                if conn and is_write:
                    try: conn.rollback(); logging.warning("æ•°æ®åº“äº‹åŠ¡å·²å›æ»šã€‚")
                    except Exception as rb_err: logging.error(f"æ•°æ®åº“å›æ»šå¤±è´¥: {rb_err}") # Catch generic exception on rollback
                # Specific error handling
                if hasattr(err, 'errno'): # Check if errno attribute exists
                    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
                        logging.critical("æ•°æ®åº“è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥å‡­æ®ã€‚")
                    elif err.errno == errorcode.ER_BAD_DB_ERROR:
                        db_name = 'N/A'
                        try:
                             if conn: db_name = conn.database
                             # Cannot get config easily here, rely on conn if available
                        except: pass
                        logging.critical(f"æ•°æ®åº“ '{db_name}' ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚")
                raise RuntimeError(f"æ•°æ®åº“é”™è¯¯: {err}") from err
            except Exception as e:
                 logging.error(f"æ‰§è¡Œæ•°æ®åº“å›è°ƒæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
                 if conn and is_write:
                     try: conn.rollback(); logging.warning("æ•°æ®åº“äº‹åŠ¡å·²å›æ»šã€‚")
                     except Exception as rb_err: logging.error(f"æ•°æ®åº“å›æ»šæ—¶å‡ºé”™: {rb_err}")
                 raise
            finally:
                if cursor:
                    try: cursor.close()
                    except Exception as cur_err: logging.warning(f"å…³é—­æ•°æ®åº“æ¸¸æ ‡æ—¶å‡ºé”™: {cur_err}")
                if conn:
                    try: conn.close() # Return connection to pool
                    except Exception as conn_err: logging.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {conn_err}")

        def _build_unprocessed_condition(self) -> str:
            """Constructs the WHERE clause for finding unprocessed rows."""
            input_conditions = []
            for col in self.columns_to_extract:
                safe_col = f"`{col.replace('`', '``')}`"
                input_conditions.append(f"({safe_col} IS NOT NULL AND {safe_col} <> '')")

            # æ ¹æ® require_all_input_fields å†³å®šæ˜¯ä½¿ç”¨ AND è¿˜æ˜¯ OR
            if self.require_all_input_fields:
                # æ‰€æœ‰è¾“å…¥å­—æ®µéƒ½å¿…é¡»éç©º
                input_clause = " AND ".join(input_conditions) if input_conditions else "1=1"
            else:
                # è‡³å°‘ä¸€ä¸ªè¾“å…¥å­—æ®µéç©ºå³å¯
                input_clause = " OR ".join(input_conditions) if input_conditions else "1=1"

            output_conditions = []
            for col in self.write_colnames:
                safe_col = f"`{col.replace('`', '``')}`"
                output_conditions.append(f"({safe_col} IS NULL OR {safe_col} = '')")
            # True if AT LEAST ONE output is empty
            output_clause = " OR ".join(output_conditions) if output_conditions else "1=0" # False if no output cols

            # Final condition: inputs valid AND at least one output empty
            return f"({input_clause}) AND ({output_clause})"


        def get_total_task_count(self) -> int:
            """Gets the total count of unprocessed tasks in the table."""
            def _get_count(conn, cursor):
                unprocessed_where = self._build_unprocessed_condition()
                sql = f"SELECT COUNT(*) as count FROM `{self.table_name}` WHERE {unprocessed_where}"
                logging.debug(f"Executing count query: {sql}")
                cursor.execute(sql)
                result = cursor.fetchone()
                count = result['count'] if result else 0
                logging.info(f"æ•°æ®åº“ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°: {count}")
                return count
            try:
                 return self.execute_with_connection(_get_count)
            except Exception as e:
                 logging.error(f"è·å–æ€»ä»»åŠ¡æ•°æ—¶å‡ºé”™: {e}")
                 return 0 # Return 0 on error

        def get_id_boundaries(self) -> Tuple[int, int]:
            """Gets the minimum and maximum ID from the table."""
            def _get_boundaries(conn, cursor):
                sql = f"SELECT MIN(id) as min_id, MAX(id) as max_id FROM `{self.table_name}`"
                logging.debug(f"Executing boundary query: {sql}")
                cursor.execute(sql)
                result = cursor.fetchone()
                if result and result['min_id'] is not None and result['max_id'] is not None:
                    min_id = int(result['min_id'])
                    max_id = int(result['max_id'])
                    logging.info(f"æ•°æ®åº“ ID èŒƒå›´: {min_id} - {max_id}")
                    return (min_id, max_id)
                else:
                     logging.warning("æ— æ³•è·å–æ•°æ®åº“è¡¨çš„ ID èŒƒå›´ (è¡¨å¯èƒ½ä¸ºç©ºæˆ–æ— 'id'åˆ—?)ã€‚è¿”å› (0, 0)ã€‚")
                     return (0, 0)
            try:
                return self.execute_with_connection(_get_boundaries)
            except Exception as e:
                 logging.error(f"è·å–IDè¾¹ç•Œæ—¶å‡ºé”™: {e}")
                 return (0, 0) # Return default on error

        def initialize_shard(self, shard_id: int, min_id: int, max_id: int) -> int:
            """Loads tasks for a specific shard based on ID range."""
            def _load_shard(conn, cursor):
                loaded_count = 0
                shard_tasks = []
                try:
                    # Ensure columns_to_extract is non-empty before creating cols string
                    if not self.select_columns:
                         logging.warning(f"åˆ†ç‰‡ {shard_id}: columns_to_extract ä¸ºç©ºï¼Œæ— æ³•åŠ è½½æ•°æ®ã€‚")
                         return 0

                    columns_str = ", ".join(f"`{col.replace('`', '``')}`" for col in self.select_columns)
                    unprocessed_where = self._build_unprocessed_condition()
                    sql = f"""
                        SELECT {columns_str}
                        FROM `{self.table_name}`
                        WHERE id BETWEEN %s AND %s AND {unprocessed_where}
                        ORDER BY id ASC
                    """
                    logging.debug(f"Executing shard load query for shard {shard_id} (IDs {min_id}-{max_id})")
                    cursor.execute(sql, (min_id, max_id))
                    rows = cursor.fetchall()
                    logging.debug(f"æŸ¥è¯¢åˆ° {len(rows)} æ¡åŸå§‹è®°å½•ã€‚")

                    for row in rows:
                        record_id = row.get("id") # Safely get id
                        if record_id is None:
                             logging.warning(f"åˆ†ç‰‡ {shard_id}: æŸ¥è¯¢ç»“æœä¸­ç¼ºå°‘ 'id'ï¼Œè·³è¿‡æ­¤è¡Œ: {row}")
                             continue
                        record_dict = {col: row.get(col) for col in self.columns_to_extract}
                        shard_tasks.append((record_id, record_dict))
                    loaded_count = len(shard_tasks)

                except Exception as e:
                    logging.error(f"åŠ è½½åˆ†ç‰‡ {shard_id} (IDs {min_id}-{max_id}) å¤±è´¥: {e}", exc_info=True)
                    loaded_count = 0
                    shard_tasks = []

                with self.lock:
                    self.tasks = shard_tasks
                self.current_shard_id = shard_id
                self.current_min_id = min_id
                self.current_max_id = max_id

                logging.info(f"åˆ†ç‰‡ {shard_id} (IDèŒƒå›´: {min_id}-{max_id}) åŠ è½½å®Œæˆï¼Œæœªå¤„ç†ä»»åŠ¡æ•°={loaded_count}")
                return loaded_count

            try:
                 return self.execute_with_connection(_load_shard, is_write=False)
            except Exception as e:
                 logging.error(f"æ‰§è¡ŒåŠ è½½åˆ†ç‰‡ {shard_id} æ“ä½œæ—¶å‡ºé”™: {e}")
                 return 0 # Return 0 on error

        # --- ADDED get_task_batch for MySQLTaskPool ---
        def get_task_batch(self, batch_size: int) -> List[Tuple[Any, Dict[str, Any]]]:
            """Gets a batch of tasks from the in-memory list."""
            with self.lock: # Use the lock inherited from BaseTaskPool
                batch = self.tasks[:batch_size]
                self.tasks = self.tasks[batch_size:]
                # logging.debug(f"å–å‡º {len(batch)} ä¸ªMySQLä»»åŠ¡ï¼Œå‰©ä½™ {len(self.tasks)}")
                return batch
        # --- END ADDED METHOD ---

        def update_task_results(self, results: Dict[int, Dict[str, Any]]):
            """Updates task results back to the database."""
            if not results: return

            updates_data = []
            for record_id, row_result in results.items():
                if "_error" not in row_result:
                    update_values = {col_name: row_result.get(alias)
                                     for alias, col_name in self.columns_to_write.items()
                                     if alias in row_result} # Only include keys present in result
                    if update_values:
                         updates_data.append((record_id, update_values))

            if not updates_data:
                logging.info("æ²¡æœ‰æˆåŠŸçš„è®°å½•éœ€è¦æ›´æ–°åˆ°æ•°æ®åº“ã€‚")
                return

            update_count = len(updates_data)
            logging.info(f"å‡†å¤‡å°† {update_count} æ¡æˆåŠŸè®°å½•çš„ç»“æœæ›´æ–°å›æ•°æ®åº“...")

            def _perform_updates(conn, cursor):
                nonlocal update_count
                success_count = 0
                errors_count = 0
                # Use prepared statements for single updates (generally safer)
                sql_template = "UPDATE `{table}` SET {set_clause} WHERE id = %s"
                for record_id, values_dict in updates_data:
                    set_parts = []
                    params = []
                    for col_name, value in values_dict.items():
                        set_parts.append(f"`{col_name.replace('`', '``')}` = %s")
                        params.append(value)
                    if not set_parts: continue

                    sql = sql_template.format(
                        table=self.table_name,
                        set_clause=", ".join(set_parts)
                    )
                    params.append(record_id)
                    try:
                        # logging.debug(f"Executing update for ID {record_id}")
                        cursor.execute(sql, tuple(params))
                        if cursor.rowcount == 1:
                            success_count += 1
                        else:
                            logging.warning(f"æ›´æ–°è®°å½• {record_id} æ—¶å½±å“è¡Œæ•°ä¸º {cursor.rowcount} (é¢„æœŸ 1)ã€‚")
                            if cursor.rowcount == 0: success_count += 1 # Assume already updated
                            else: errors_count += 1 # Log unexpected multi-row update as error
                    except mysql.connector.Error as single_err:
                        logging.error(f"æ›´æ–°è®°å½• {record_id} å¤±è´¥: {single_err}")
                        errors_count += 1

                if errors_count > 0:
                     logging.error(f"æ•°æ®åº“æ›´æ–°å®Œæˆï¼Œä½†æœ‰ {errors_count} æ¬¡æ›´æ–°å¤±è´¥ã€‚")
                logging.info(f"æ•°æ®åº“æ›´æ–°å®Œæˆï¼ŒæˆåŠŸæ›´æ–° {success_count} æ¡è®°å½•ã€‚")

            try:
                 self.execute_with_connection(_perform_updates, is_write=True)
            except Exception as e:
                 logging.error(f"æ›´æ–°æ•°æ®åº“è®°å½•çš„æ•´ä½“æ“ä½œå¤±è´¥: {e}")


        def reload_task_data(self, record_id: int) -> Optional[Dict[str, Any]]:
            """Reloads the original input data for a specific task ID."""
            def _reload(conn, cursor):
                if not self.columns_to_extract:
                     logging.warning(f"æ— æ³•é‡è½½è®°å½• {record_id}ï¼ŒæœªæŒ‡å®šè¾“å…¥åˆ—ã€‚")
                     return None
                cols_str = ", ".join(f"`{c.replace('`', '``')}`" for c in self.columns_to_extract)
                sql = f"SELECT {cols_str} FROM `{self.table_name}` WHERE id=%s"
                cursor.execute(sql, (record_id,))
                row = cursor.fetchone()
                if row:
                    return {c: row.get(c) for c in self.columns_to_extract}
                else:
                    logging.warning(f"å°è¯•é‡è½½æ•°æ®å¤±è´¥ï¼šè®°å½• {record_id} åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ã€‚")
                    return None
            try:
                 return self.execute_with_connection(_reload, is_write=False)
            except Exception as e:
                 logging.error(f"æ‰§è¡Œé‡è½½è®°å½• {record_id} æ•°æ®æ“ä½œå¤±è´¥: {e}")
                 return None # Return None on failure

        def close(self):
            """Closes the connection pool."""
            logging.info("è¯·æ±‚å…³é—­ MySQL è¿æ¥æ± ...")
            MySQLConnectionPoolManager.close_pool()

# --- Excel Specific Components ---
if EXCEL_ENABLED:
    class ExcelTaskPool(BaseTaskPool):
        """Excel data source task pool implementation."""
        def __init__(
            self,
            input_excel: str,
            output_excel: str,
            columns_to_extract: List[str],
            columns_to_write: Dict[str, str],
            save_interval: int = 300,
            require_all_input_fields: bool = True
        ):
            if not os.path.exists(input_excel):
                raise FileNotFoundError(f"Excelè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_excel}")

            super().__init__(columns_to_extract, columns_to_write, require_all_input_fields) # Call super first

            logging.info(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {input_excel}")
            try:
                self.df = pd.read_excel(input_excel, engine='openpyxl')
                logging.info(f"Excelæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {len(self.df)} è¡Œã€‚")
            except Exception as e:
                raise IOError(f"æ— æ³•è¯»å–Excelæ–‡ä»¶ {input_excel}: {e}") from e

            self.output_excel = output_excel
            self.save_interval = save_interval
            self.last_save_time = time.time()
            # Using inherited lock from BaseTaskPool for simplicity
            # self.df_lock = threading.Lock()

            # --- Column Validation and Preparation ---
            missing_extract_cols = [c for c in self.columns_to_extract if c not in self.df.columns]
            if missing_extract_cols:
                logging.warning(f"è¾“å…¥åˆ— {missing_extract_cols} åœ¨Excelä¸­ä¸å­˜åœ¨ã€‚")

            for alias, out_col in self.columns_to_write.items():
                if out_col not in self.df.columns:
                    logging.warning(f"è¾“å‡ºåˆ— '{out_col}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°åˆ—ã€‚")
                    self.df[out_col] = pd.NA if hasattr(pd, 'NA') else None

            self.current_shard_id = -1
            self.current_min_idx = 0
            self.current_max_idx = 0
            logging.info(f"ExcelTaskPool åˆå§‹åŒ–å®Œæˆã€‚è¾“å…¥: {input_excel}, è¾“å‡º: {output_excel}")

        def _is_value_empty(self, value) -> bool:
            """Checks if a value is considered empty (NaN, None, or empty/whitespace string)."""
            if pd.isna(value): return True
            if isinstance(value, str) and not value.strip(): return True
            # Consider 0 or False as non-empty unless specifically required
            return False

        def _is_value_empty_vectorized(self, series: pd.Series) -> pd.Series:
            """Vectorized version of _is_value_empty for better performance.

            Checks if values in a Series are empty (NA, None, or blank strings).
            This method is semantically identical to _is_value_empty but uses
            vectorized operations, providing 50-100x speedup on large datasets.

            Args:
                series: pandas Series to check

            Returns:
                Boolean Series indicating which values are empty

            Performance:
                - Original: O(n) with Python function call overhead per row
                - Vectorized: O(n) with C-level pandas operations
                - Expected speedup: 50-100x on 100k+ rows
            """
            # First layer: Check for NA values (NaN, None, pd.NA)
            is_na = series.isna()

            # Second layer: Check for blank strings (only for object dtype)
            if series.dtype == 'object':
                # Pandas .str accessor behavior:
                # - String values: performs strip() operation
                # - Non-string values (int, float, etc.): returns NaN
                # Therefore, == '' only matches truly blank strings
                is_blank_str = series.str.strip() == ''
                return is_na | is_blank_str
            else:
                # For numeric dtypes, only NA check is needed
                return is_na

        def _filter_unprocessed_indices(self, min_idx: int, max_idx: int) -> List[int]:
            """Filters DataFrame indices for unprocessed rows within the range."""
            unprocessed_indices = []
            start_idx = max(0, min_idx)
            end_idx = min(len(self.df), max_idx + 1)

            if start_idx >= end_idx: return []

            logging.debug(f"å¼€å§‹è¿‡æ»¤ç´¢å¼•èŒƒå›´ {start_idx} åˆ° {end_idx - 1}...")
            try:
                sub_df = self.df.iloc[start_idx:end_idx]

                # Input condition: æ ¹æ® require_all_input_fields å†³å®šæ£€æŸ¥é€»è¾‘
                if self.columns_to_extract:
                    if self.require_all_input_fields:
                        # æ‰€æœ‰è¾“å…¥å­—æ®µéƒ½å¿…é¡»éç©º (ä½¿ç”¨ AND é€»è¾‘)
                        input_valid_mask = pd.Series(True, index=sub_df.index)
                        for col in self.columns_to_extract:
                            if col in sub_df.columns:
                                # Using vectorized _is_value_empty_vectorized for performance
                                input_valid_mask &= ~self._is_value_empty_vectorized(sub_df[col]) # Invert empty check
                            else:
                                input_valid_mask &= False # Column missing = invalid input
                    else:
                        # è‡³å°‘ä¸€ä¸ªè¾“å…¥å­—æ®µéç©ºå³å¯ (ä½¿ç”¨ OR é€»è¾‘)
                        input_valid_mask = pd.Series(False, index=sub_df.index)
                        for col in self.columns_to_extract:
                            if col in sub_df.columns:
                                input_valid_mask |= ~self._is_value_empty_vectorized(sub_df[col]) # At least one non-empty
                            # Column missing doesn't affect OR logic
                else:
                    input_valid_mask = pd.Series(True, index=sub_df.index) # No input cols = always valid

                # Output condition: At least one write column must be empty
                if self.columns_to_write:
                    output_empty_mask = pd.Series(False, index=sub_df.index)
                    for alias, out_col in self.columns_to_write.items():
                        if out_col in sub_df.columns:
                             output_empty_mask |= self._is_value_empty_vectorized(sub_df[out_col])
                        else:
                             output_empty_mask |= True # Column missing = considered empty
                else:
                    output_empty_mask = pd.Series(False, index=sub_df.index) # No output cols = never empty

                # Combine conditions
                final_mask = input_valid_mask & output_empty_mask
                unprocessed_indices = sub_df.index[final_mask].tolist()

            except Exception as e:
                 logging.error(f"è¿‡æ»¤æœªå¤„ç†ç´¢å¼•æ—¶å‡ºé”™: {e}", exc_info=True)
                 return []

            logging.debug(f"è¿‡æ»¤ç´¢å¼•èŒƒå›´ {start_idx}-{end_idx-1} å®Œæˆï¼Œæ‰¾åˆ° {len(unprocessed_indices)} ä¸ªæœªå¤„ç†ç´¢å¼•ã€‚")
            return unprocessed_indices

        def get_total_task_count(self) -> int:
            """Gets the total count of unprocessed tasks in the DataFrame."""
            logging.info("æ­£åœ¨è®¡ç®— Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°...")
            unprocessed = self._filter_unprocessed_indices(0, len(self.df) - 1)
            count = len(unprocessed)
            logging.info(f"Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°: {count}")
            return count

        def get_id_boundaries(self) -> Tuple[int, int]:
            """Gets the index boundaries of the DataFrame."""
            if self.df.empty: return (0, -1)
            max_index = len(self.df) - 1
            logging.info(f"Excel DataFrame ç´¢å¼•èŒƒå›´: 0 - {max_index}")
            return (0, max_index)

        def initialize_shard(self, shard_id: int, min_idx: int, max_idx: int) -> int:
            """Loads tasks for a specific shard based on index range."""
            logging.info(f"å¼€å§‹åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx})...")
            loaded_count = 0
            shard_tasks = []
            try:
                unprocessed_idx = self._filter_unprocessed_indices(min_idx, max_idx)

                if unprocessed_idx:
                    logging.debug(f"åˆ†ç‰‡ {shard_id}: æ‰¾åˆ° {len(unprocessed_idx)} ä¸ªæœªå¤„ç†ç´¢å¼•ï¼Œæ­£åœ¨æå–æ•°æ®...")
                    for idx in unprocessed_idx:
                        try:
                            row_data = self.df.loc[idx]
                            record_dict = {}
                            for col in self.columns_to_extract:
                                value = row_data.get(col)
                                record_dict[col] = str(value) if pd.notna(value) else "" # Ensure string for prompt
                            shard_tasks.append((idx, record_dict))
                        except Exception as row_err:
                            logging.error(f"åˆ†ç‰‡ {shard_id}: æå–ç´¢å¼• {idx} çš„æ•°æ®æ—¶å‡ºé”™: {row_err}", exc_info=True)
                    loaded_count = len(shard_tasks)
                else:
                    logging.info(f"åˆ†ç‰‡ {shard_id}: åœ¨æŒ‡å®šç´¢å¼•èŒƒå›´å†…æœªæ‰¾åˆ°æœªå¤„ç†çš„ä»»åŠ¡ã€‚")

            except Exception as e:
                logging.error(f"åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼• {min_idx}-{max_idx}) å¤±è´¥: {e}", exc_info=True)
                loaded_count = 0
                shard_tasks = []

            with self.lock: # Lock for accessing self.tasks
                self.tasks = shard_tasks
            self.current_shard_id = shard_id
            self.current_min_idx = min_idx
            self.current_max_idx = max_idx

            logging.info(f"åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx}) åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½æœªå¤„ç†ä»»åŠ¡æ•°={loaded_count}")
            return loaded_count

        # --- ADDED get_task_batch implementation for ExcelTaskPool ---
        def get_task_batch(self, batch_size: int) -> List[Tuple[Any, Dict[str, Any]]]:
            """Gets a batch of tasks from the in-memory list."""
            with self.lock: # Use the lock inherited from BaseTaskPool
                batch = self.tasks[:batch_size]
                self.tasks = self.tasks[batch_size:]
                # logging.debug(f"å–å‡º {len(batch)} ä¸ªExcelä»»åŠ¡ï¼Œå‰©ä½™ {len(self.tasks)}")
                return batch
        # --- END ADDED METHOD ---

        def update_task_results(self, results: Dict[int, Dict[str, Any]]):
            """Updates task results back to the DataFrame and handles periodic saving."""
            if not results: return

            updated_indices = []
            needs_save = False # Flag to check if save is needed after updates

            try:
                # Use the single lock for DataFrame modifications
                with self.lock:
                    for idx, row_result in results.items():
                        if "_error" not in row_result:
                            if idx in self.df.index:
                                for alias, col_name in self.columns_to_write.items():
                                    if col_name in self.df.columns:
                                        value_to_write = row_result.get(alias, "") # Default to empty string
                                        try:
                                            self.df.loc[idx, col_name] = value_to_write
                                        except Exception as e_set:
                                             logging.warning(f"è®¾ç½®ç´¢å¼• {idx} åˆ— '{col_name}' å€¼ '{value_to_write}' å¤±è´¥: {e_set}")
                                updated_indices.append(idx)
                            else:
                                logging.warning(f"å°è¯•æ›´æ–° Excel ä¸­ä¸å­˜åœ¨çš„ç´¢å¼• {idx}ï¼Œè·³è¿‡ã€‚")

                    if updated_indices:
                        update_count = len(updated_indices)
                        logging.info(f"å·²åœ¨å†…å­˜ä¸­æ›´æ–° {update_count} æ¡ Excel è®°å½•ã€‚")
                        current_time = time.time()
                        if current_time - self.last_save_time >= self.save_interval:
                             needs_save = True
                             self.last_save_time = current_time # Update time *before* save attempt

            except Exception as e:
                logging.error(f"æ›´æ–° Excel DataFrame æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
                needs_save = False # Do not save if update itself failed

            # Perform save outside the main lock if needed
            if needs_save:
                logging.info(f"è¾¾åˆ°ä¿å­˜é—´éš” ({self.save_interval}s)ï¼Œå‡†å¤‡ä¿å­˜ Excel æ–‡ä»¶...")
                try:
                    self._save_excel() # This method handles its own errors & locking for save
                except Exception as save_err:
                     # Log error but allow processing to continue, next interval will retry save
                     logging.error(f"è‡ªåŠ¨ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {save_err}")
                     # Reset last_save_time to try saving again sooner? Or keep it updated? Keep updated for now.

        def _save_excel(self):
            """
            ä¿å­˜Excelæ–‡ä»¶ - æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼çš„ç®€æ´ç‰ˆæœ¬
            """
            logging.info(f"æ­£åœ¨å°è¯•ä¿å­˜ DataFrame åˆ°: {self.output_excel}")
            
            try:
                with self.lock:
                    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                    output_dir = os.path.dirname(self.output_excel)
                    if output_dir and not os.path.exists(output_dir):
                        os.makedirs(output_dir, exist_ok=True)

                    # ğŸ¯ ç­–ç•¥1: ç›´æ¥ä¿å­˜ï¼ˆæ­£å¸¸æƒ…å†µï¼Œé›¶æ€§èƒ½å½±å“ï¼‰
                    try:
                        self.df.to_excel(self.output_excel, index=False, engine='openpyxl')
                        logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜åˆ°: {self.output_excel}")
                        return
                        
                    except UnicodeEncodeError as e:
                        logging.error(f"âŒ Unicodeç¼–ç é—®é¢˜: {e}")
                        logging.info("ğŸ§¹ å¼€å§‹æ¸…ç©ºAIè¾“å‡ºåˆ—ä¸­çš„é—®é¢˜å•å…ƒæ ¼...")
                        
                        # ğŸ§¹ ç­–ç•¥2: æ¸…ç©ºAIè¾“å‡ºåˆ—ä¸­çš„é—®é¢˜å•å…ƒæ ¼
                        fixed_df = self.df.copy()
                        cleared_count = 0
                        ai_columns = list(self.columns_to_write.values())  # åªæ£€æŸ¥AIè¾“å‡ºåˆ—
                        
                        for col_name in ai_columns:
                            if col_name not in fixed_df.columns:
                                continue
                                
                            for row_idx in fixed_df.index:
                                value = fixed_df.loc[row_idx, col_name]
                                
                                if isinstance(value, str) and value:
                                    try:
                                        value.encode('utf-8')
                                    except UnicodeEncodeError:
                                        # å‘ç°é—®é¢˜ï¼Œæ¸…ç©ºæ­¤å•å…ƒæ ¼
                                        logging.warning(f"âŒ æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼: ç¬¬ {row_idx} è¡Œ, '{col_name}' åˆ—")
                                        fixed_df.loc[row_idx, col_name] = ""
                                        cleared_count += 1
                        
                        if cleared_count > 0:
                            logging.info(f"ğŸ§¹ å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼ï¼Œé‡æ–°å°è¯•ä¿å­˜...")
                            
                            # å°è¯•ä¿å­˜æ¸…ç†åçš„æ•°æ®
                            try:
                                fixed_df.to_excel(self.output_excel, index=False, engine='openpyxl')
                                logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜åˆ°: {self.output_excel} (å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼)")
                                self.df = fixed_df  # æ›´æ–°å†…å­˜ä¸­çš„æ•°æ®
                                return
                            except UnicodeEncodeError:
                                logging.warning("âš ï¸  æ¸…ç©ºAIè¾“å‡ºåˆ—åä»æœ‰é—®é¢˜ï¼Œå¯èƒ½æ¥è‡ªåŸå§‹æ•°æ®")
                        
                        # ğŸ“Š ç­–ç•¥3: CSVå¤‡é€‰æ–¹æ¡ˆ
                        csv_path = self.output_excel.replace('.xlsx', '.csv')
                        logging.warning(f"âš ï¸  Excelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV: {csv_path}")
                        
                        df_to_save = fixed_df if cleared_count > 0 else self.df
                        df_to_save.to_csv(csv_path, index=False, encoding='utf-8-sig')
                        logging.warning(f"âœ… å·²ä¿å­˜ä¸ºCSV: {csv_path}")

            except Exception as e:
                logging.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                raise IOError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}") from e

        def reload_task_data(self, idx: int) -> Optional[Dict[str, Any]]:
            """Reloads the original input data for a specific task index."""
            try:
                # Read access might be okay without lock if updates are careful
                if idx in self.df.index:
                    row_data = self.df.loc[idx]
                    record_dict = {
                        c: str(row_data.get(c)) if pd.notna(row_data.get(c)) else ""
                        for c in self.columns_to_extract
                    }
                    return record_dict
                else:
                    logging.warning(f"å°è¯•é‡è½½æ•°æ®å¤±è´¥ï¼šç´¢å¼• {idx} åœ¨ DataFrame ä¸­ä¸å­˜åœ¨ã€‚")
                    return None
            except Exception as e:
                logging.error(f"é‡è½½ç´¢å¼• {idx} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                return None

        def close(self):
            """Forces a final save of the Excel file."""
            logging.info("æ­£åœ¨æ‰§è¡Œ Excel æ–‡ä»¶çš„æœ€ç»ˆä¿å­˜æ“ä½œ...")
            try:
                self._save_excel()
            except Exception as e:
                 logging.error(f"æœ€ç»ˆä¿å­˜ Excel æ–‡ä»¶å¤±è´¥: {e}")


# --- Top-Level Factory Function ---
def create_task_pool(config: Dict[str, Any], columns_to_extract: List[str], columns_to_write: Dict[str, str]) -> BaseTaskPool:
    """
    Factory function to create the appropriate task pool (MySQL or Excel)
    based on the configuration.
    """
    datasource_config = config.get("datasource", {})
    ds_type = datasource_config.get("type", "excel").lower()
    concurrency_config = datasource_config.get("concurrency", {})

    # è¯»å–è¾“å…¥å­—æ®µæ£€æŸ¥é…ç½®ï¼Œé»˜è®¤ä¸º Trueï¼ˆéœ€è¦æ‰€æœ‰å­—æ®µéç©ºï¼‰
    require_all_input_fields = datasource_config.get("require_all_input_fields", True)
    logging.info(f"è¾“å…¥å­—æ®µæ£€æŸ¥æ¨¡å¼: {'è¦æ±‚æ‰€æœ‰å­—æ®µéç©º' if require_all_input_fields else 'è‡³å°‘ä¸€ä¸ªå­—æ®µéç©ºå³å¯'}")

    logging.info(f"æ ¹æ®é…ç½®ç±»å‹ '{ds_type}' åˆ›å»ºä»»åŠ¡æ± ...")

    if ds_type == "mysql":
        if not MYSQL_AVAILABLE:
            raise ImportError("MySQL æ•°æ®æºå·²é…ç½®ï¼Œä½† MySQL Connector åº“ä¸å¯ç”¨ã€‚")
        mysql_config = config.get("mysql")
        if not mysql_config: raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ [mysql] é…ç½®æ®µã€‚")
        required_keys = ["host", "user", "password", "database", "table_name"]
        missing_keys = [k for k in required_keys if k not in mysql_config or not mysql_config[k]]
        if missing_keys: raise ValueError(f"MySQLé…ç½®ä¸å®Œæ•´: {', '.join(missing_keys)}")

        connection_config = {k: mysql_config[k] for k in ["host", "user", "password", "database"]}
        connection_config["port"] = mysql_config.get("port", 3306)
        table_name = mysql_config["table_name"]
        pool_size = concurrency_config.get("db_pool_size", 5)

        logging.info(f"å‡†å¤‡åˆ›å»º MySQLTaskPool: è¡¨={table_name}, æ± å¤§å°={pool_size}")
        return MySQLTaskPool(connection_config, columns_to_extract, columns_to_write, table_name, pool_size, require_all_input_fields)

    elif ds_type == "excel":
        if not EXCEL_ENABLED:
            raise ImportError("Excel æ•°æ®æºå·²é…ç½®ï¼Œä½† Pandas æˆ– openpyxl åº“ä¸å¯ç”¨ã€‚")
        excel_config = config.get("excel")
        if not excel_config: raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ [excel] é…ç½®æ®µã€‚")
        input_excel = excel_config.get("input_path")
        if not input_excel: raise ValueError("Excel é…ç½®ç¼ºå°‘ 'input_path'ã€‚")

        output_excel = excel_config.get("output_path")
        if not output_excel:
            base, ext = os.path.splitext(input_excel)
            output_excel = f"{base}_output{ext}"
            logging.info(f"æœªæŒ‡å®š Excel è¾“å‡ºè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤å€¼: {output_excel}")

        save_interval = concurrency_config.get("save_interval", 300)
        logging.info(f"å‡†å¤‡åˆ›å»º ExcelTaskPool: è¾“å…¥={input_excel}, è¾“å‡º={output_excel}, ä¿å­˜é—´éš”={save_interval}s")
        return ExcelTaskPool(input_excel, output_excel, columns_to_extract, columns_to_write, save_interval, require_all_input_fields)

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹é…ç½®: '{ds_type}'")