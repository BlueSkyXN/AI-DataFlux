"""
Excel æ•°æ®æºä»»åŠ¡æ± å®ç°æ¨¡å—

æœ¬æ¨¡å—æä¾›åŸºäº DataFrame çš„ Excel/CSV æ–‡ä»¶ä»»åŠ¡æ± å®ç°ã€‚
é€šè¿‡æŠ½è±¡å¼•æ“å±‚ï¼Œç»Ÿä¸€æ”¯æŒ Pandas å’Œ Polars ä¸¤ç§ DataFrame æ¡†æ¶ï¼Œ
å¹¶å¯é€‰ç”¨é«˜æ€§èƒ½è¯»å†™å™¨æå‡ I/O æ•ˆç‡ã€‚

æ ¸å¿ƒç‰¹æ€§:
    - å¤šå¼•æ“æ”¯æŒ: Pandas (å…¼å®¹æ€§å¥½) å’Œ Polars (é«˜æ€§èƒ½)
    - é«˜æ€§èƒ½è¯»å–: å¯é€‰ calamine (Rust) å¼•æ“ï¼Œæ¯” openpyxl å¿« 10 å€
    - é«˜æ€§èƒ½å†™å…¥: å¯é€‰ xlsxwriterï¼Œæ¯” openpyxl å¿« 3 å€
    - å‘é‡åŒ–è¿‡æ»¤: ä½¿ç”¨ DataFrame åŸç”Ÿæ“ä½œï¼Œé¿å…é€è¡Œéå†
    - è‡ªåŠ¨ä¿å­˜: å®šæ—¶æŒä¹…åŒ–ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
    - ç¼–ç ä¿®å¤: è‡ªåŠ¨å¤„ç† Unicode ç¼–ç é—®é¢˜
    - CSV å…¼å®¹: è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹ï¼ŒåŒä¸€æ¥å£å¤„ç† Excel å’Œ CSV

æ¶æ„è®¾è®¡:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              ExcelTaskPool                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ ä»»åŠ¡é˜Ÿåˆ—    â”‚   â”‚ DataFrame (å¼•æ“ç‰¹å®š)    â”‚  â”‚
    â”‚  â”‚ tasks[]     â”‚   â”‚ df                       â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚         â”‚                    â”‚                   â”‚
    â”‚         â–¼                    â–¼                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚            BaseEngine (æŠ½è±¡å±‚)              â”‚â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚â”‚
    â”‚  â”‚  â”‚ PandasEngine â”‚    â”‚ PolarsEngine â”‚      â”‚â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ€§èƒ½ä¼˜åŒ–:
    1. å‘é‡åŒ–è¿‡æ»¤: filter_indices_vectorized() æ¯”é€è¡Œå¿« 50-100 å€
    2. æ‰¹é‡æ›´æ–°: set_values_batch() å‡å°‘å†…å­˜åˆ†é…
    3. å»¶è¿Ÿä¿å­˜: save_interval æœºåˆ¶å‡å°‘ç£ç›˜ I/O
    4. é«˜æ€§èƒ½åº“: calamine + xlsxwriter ç»„åˆæœ€ä¼˜

ä½¿ç”¨ç¤ºä¾‹:
    from src.data.excel import ExcelTaskPool
    
    pool = ExcelTaskPool(
        input_path="data/input.xlsx",
        output_path="data/output.xlsx",
        columns_to_extract=["title", "content"],
        columns_to_write={"result": "ai_result", "score": "ai_score"},
        save_interval=300,  # 5åˆ†é’Ÿè‡ªåŠ¨ä¿å­˜
        engine_type="auto",  # è‡ªåŠ¨é€‰æ‹©å¼•æ“
        excel_reader="calamine",  # ä½¿ç”¨é«˜æ€§èƒ½è¯»å–å™¨
        excel_writer="xlsxwriter",  # ä½¿ç”¨é«˜æ€§èƒ½å†™å…¥å™¨
    )
    
    # è·å–ä»»åŠ¡æ‰¹æ¬¡
    batch = pool.get_task_batch(100)
    
    # å¤„ç†åæ›´æ–°ç»“æœ
    results = {0: {"result": "åˆ†æç»“æœ", "score": "0.95"}}
    pool.update_task_results(results)
    
    # å…³é—­å¹¶ä¿å­˜
    pool.close()

é…ç½®é€‰é¡¹:
    engine_type: "pandas" | "polars" | "auto"
        - pandas: å…¼å®¹æ€§æœ€å¥½ï¼Œå†…å­˜å ç”¨è¾ƒé«˜
        - polars: é«˜æ€§èƒ½ï¼Œå¤šçº¿ç¨‹ï¼Œå†…å­˜æ•ˆç‡é«˜
        - auto: ä¼˜å…ˆ polarsï¼Œä¸å¯ç”¨æ—¶å›é€€ pandas
    
    excel_reader: "openpyxl" | "calamine" | "auto"
        - openpyxl: çº¯ Pythonï¼ŒåŠŸèƒ½å®Œæ•´
        - calamine: Rust å®ç°ï¼Œé€Ÿåº¦ 10xï¼Œä»…æ”¯æŒè¯»å–
        - auto: ä¼˜å…ˆ calamine
    
    excel_writer: "openpyxl" | "xlsxwriter" | "auto"
        - openpyxl: æ”¯æŒè¯»å†™ï¼ŒåŠŸèƒ½å®Œæ•´
        - xlsxwriter: ä»…å†™å…¥ï¼Œé€Ÿåº¦ 3xï¼Œæ ¼å¼æ”¯æŒæ›´å¥½
        - auto: ä¼˜å…ˆ xlsxwriter

æ³¨æ„äº‹é¡¹:
    1. å¤§æ–‡ä»¶ (>100MB) å»ºè®®ä½¿ç”¨ polars + calamine
    2. éœ€è¦ä¿ç•™æ ¼å¼æ—¶ä½¿ç”¨ openpyxl
    3. CSV æ–‡ä»¶ä¸éœ€è¦ excel_reader/excel_writer é…ç½®
    4. è‡ªåŠ¨ä¿å­˜åœ¨é”å¤–æ‰§è¡Œï¼Œé¿å…é˜»å¡
"""

import logging
import time
from pathlib import Path
from typing import Any

from .base import BaseTaskPool
from .engines import get_engine, BaseEngine


class ExcelTaskPool(BaseTaskPool):
    """
    Excel/CSV æ•°æ®æºä»»åŠ¡æ± 
    
    ä» Excel æˆ– CSV æ–‡ä»¶è¯»å–ä»»åŠ¡æ•°æ®ï¼ŒAI å¤„ç†åå†™å›ç»“æœã€‚
    æ ¸å¿ƒèŒè´£æ˜¯ç®¡ç†å†…å­˜ä¸­çš„ DataFrame å’Œä»»åŠ¡é˜Ÿåˆ—çš„åŒæ­¥ã€‚
    
    å·¥ä½œæµç¨‹:
        1. åˆå§‹åŒ–: è¯»å–æ–‡ä»¶ â†’ éªŒè¯åˆ— â†’ åˆ›å»ºè¾“å‡ºåˆ—
        2. åˆ†ç‰‡åŠ è½½: è¿‡æ»¤æœªå¤„ç†ç´¢å¼• â†’ æå–æ•°æ® â†’ å¡«å……ä»»åŠ¡é˜Ÿåˆ—
        3. ä»»åŠ¡è·å–: ä»é˜Ÿåˆ—å¼¹å‡ºæ‰¹æ¬¡
        4. ç»“æœæ›´æ–°: å†™å…¥ DataFrame â†’ æ£€æŸ¥ä¿å­˜é—´éš”
        5. å…³é—­: æ‰§è¡Œæœ€ç»ˆä¿å­˜

    Attributes:
        input_path (Path): è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path (Path): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        save_interval (int): è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆç§’ï¼‰
        last_save_time (float): ä¸Šæ¬¡ä¿å­˜æ—¶é—´æˆ³
        engine (BaseEngine): DataFrame å¼•æ“å®ä¾‹
        df: å½“å‰ DataFrameï¼ˆPandas æˆ– Polarsï¼‰
        _is_csv (bool): æ˜¯å¦ä¸º CSV æ–‡ä»¶
        current_shard_id (int): å½“å‰åˆ†ç‰‡ ID
        current_min_idx (int): å½“å‰åˆ†ç‰‡æœ€å°ç´¢å¼•
        current_max_idx (int): å½“å‰åˆ†ç‰‡æœ€å¤§ç´¢å¼•
    
    çº¿ç¨‹å®‰å…¨:
        - ä½¿ç”¨ self.lock ä¿æŠ¤ DataFrame å’Œä»»åŠ¡é˜Ÿåˆ—
        - ä¿å­˜æ“ä½œåœ¨é”å¤–æ‰§è¡Œï¼ˆé¿å…é•¿æ—¶é—´é˜»å¡ï¼‰
    """

    def __init__(
        self,
        input_path: str | Path,
        output_path: str | Path,
        columns_to_extract: list[str],
        columns_to_write: dict[str, str],
        save_interval: int = 300,
        require_all_input_fields: bool = True,
        engine_type: str = "pandas",
        excel_reader: str = "auto",
        excel_writer: str = "auto",
    ):
        """
        åˆå§‹åŒ– Excel/CSV ä»»åŠ¡æ± 
        
        åˆ›å»ºæµç¨‹:
            1. éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
            2. åˆå§‹åŒ–åŸºç±»ï¼ˆè®¾ç½®åˆ—é…ç½®ï¼‰
            3. è·å–å¹¶é…ç½® DataFrame å¼•æ“
            4. æ£€æµ‹æ–‡ä»¶ç±»å‹ï¼ˆCSV æˆ– Excelï¼‰
            5. è¯»å–æ–‡ä»¶åˆ°å†…å­˜
            6. éªŒè¯å’Œå‡†å¤‡åˆ—

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆExcel æˆ– CSVï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä¸è¾“å…¥ç›¸åŒï¼ŒåŸåœ°ä¿®æ”¹ï¼‰
            columns_to_extract: éœ€è¦æå–çš„è¾“å…¥åˆ—ååˆ—è¡¨
                ä¾‹: ["title", "content", "category"]
            columns_to_write: AI è¾“å‡ºå­—æ®µæ˜ å°„ {åˆ«å: å®é™…åˆ—å}
                ä¾‹: {"result": "ai_result", "confidence": "ai_confidence"}
            save_interval: è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 300ï¼ˆ5åˆ†é’Ÿï¼‰
                è®¾ä¸º 0 ç¦ç”¨è‡ªåŠ¨ä¿å­˜ï¼ˆä¸æ¨èï¼‰
            require_all_input_fields: æ˜¯å¦è¦æ±‚æ‰€æœ‰è¾“å…¥å­—æ®µéƒ½éç©º
                - True: æ‰€æœ‰è¾“å…¥åˆ—éƒ½æœ‰å€¼æ‰è§†ä¸ºæœ‰æ•ˆä»»åŠ¡
                - False: ä»»ä¸€è¾“å…¥åˆ—æœ‰å€¼å³ä¸ºæœ‰æ•ˆä»»åŠ¡
            engine_type: DataFrame å¼•æ“ç±»å‹
                - "pandas": ä½¿ç”¨ Pandasï¼ˆå…¼å®¹æ€§å¥½ï¼‰
                - "polars": ä½¿ç”¨ Polarsï¼ˆé«˜æ€§èƒ½ï¼‰
                - "auto": ä¼˜å…ˆ Polarsï¼Œä¸å¯ç”¨æ—¶å›é€€ Pandas
            excel_reader: Excel è¯»å–å™¨ï¼ˆä»…å¯¹ .xlsx æœ‰æ•ˆï¼‰
                - "openpyxl": çº¯ Python å®ç°
                - "calamine": Rust é«˜æ€§èƒ½å®ç°ï¼ˆéœ€å®‰è£… python-calamineï¼‰
                - "auto": ä¼˜å…ˆ calamine
            excel_writer: Excel å†™å…¥å™¨ï¼ˆä»…å¯¹ .xlsx æœ‰æ•ˆï¼‰
                - "openpyxl": æ”¯æŒè¯»å†™
                - "xlsxwriter": ä»…å†™å…¥ï¼Œæ€§èƒ½æ›´å¥½
                - "auto": ä¼˜å…ˆ xlsxwriter

        Raises:
            FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
            IOError: æ–‡ä»¶è¯»å–å¤±è´¥ï¼ˆæ ¼å¼é”™è¯¯ã€ç¼–ç é—®é¢˜ç­‰ï¼‰
            KeyError: æŒ‡å®šçš„åˆ—åœ¨æ–‡ä»¶ä¸­ä¸å­˜åœ¨
        
        ç¤ºä¾‹:
            # åŸºæœ¬ç”¨æ³•
            pool = ExcelTaskPool(
                input_path="data/tasks.xlsx",
                output_path="data/results.xlsx",
                columns_to_extract=["title"],
                columns_to_write={"result": "ai_result"},
            )
            
            # é«˜æ€§èƒ½é…ç½®
            pool = ExcelTaskPool(
                input_path="big_data.xlsx",
                output_path="big_data_out.xlsx",
                columns_to_extract=["content"],
                columns_to_write={"summary": "ai_summary"},
                engine_type="polars",
                excel_reader="calamine",
                excel_writer="xlsxwriter",
                save_interval=60,  # å¤§æ–‡ä»¶å»ºè®®æ›´é¢‘ç¹ä¿å­˜
            )
        """
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        self.input_path = Path(input_path)
        self.output_path = Path(output_path)

        if not self.input_path.exists():
            raise FileNotFoundError(f"Excel è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_path}")

        # åˆå§‹åŒ–åŸºç±»
        super().__init__(columns_to_extract, columns_to_write, require_all_input_fields)

        # è·å– DataFrame å¼•æ“ (æ”¯æŒé«˜æ€§èƒ½è¯»å†™å™¨é…ç½®)
        self.engine: BaseEngine = get_engine(
            engine_type=engine_type,
            excel_reader=excel_reader,
            excel_writer=excel_writer,
        )
        logging.info(f"ä½¿ç”¨ DataFrame å¼•æ“: {self.engine.name}")

        # æ˜¾ç¤ºè¯»å†™å™¨ä¿¡æ¯
        if hasattr(self.engine, "excel_reader"):
            logging.info(f"  - Excel è¯»å–å™¨: {self.engine.excel_reader}")
        if hasattr(self.engine, "excel_writer"):
            logging.info(f"  - Excel å†™å…¥å™¨: {self.engine.excel_writer}")

        # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹ï¼ˆCSV æˆ– Excelï¼‰
        self._is_csv = self.input_path.suffix.lower() == ".csv"

        # è¯»å–æ–‡ä»¶ï¼ˆCSV æˆ– Excelï¼‰
        file_type = "CSV" if self._is_csv else "Excel"
        logging.info(f"æ­£åœ¨è¯»å– {file_type} æ–‡ä»¶: {self.input_path}")
        try:
            if self._is_csv:
                self.df = self.engine.read_csv(self.input_path)
            else:
                self.df = self.engine.read_excel(self.input_path)
            row_count = self.engine.row_count(self.df)
            logging.info(f"{file_type} æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {row_count} è¡Œ")
        except Exception as e:
            raise IOError(f"æ— æ³•è¯»å– {file_type} æ–‡ä»¶ {self.input_path}: {e}") from e

        # ä¿å­˜ç›¸å…³
        self.save_interval = save_interval
        self.last_save_time = time.time()

        # åˆ†ç‰‡çŠ¶æ€
        self.current_shard_id = -1
        self.current_min_idx = 0
        self.current_max_idx = 0

        # åˆ—éªŒè¯å’Œå‡†å¤‡
        self._validate_and_prepare_columns()

        logging.info(
            f"ExcelTaskPool åˆå§‹åŒ–å®Œæˆ | è¾“å…¥: {self.input_path}, è¾“å‡º: {self.output_path}"
        )

    def _validate_and_prepare_columns(self) -> None:
        """
        éªŒè¯å’Œå‡†å¤‡ DataFrame åˆ—
        
        æ‰§è¡Œä¸¤é¡¹æ£€æŸ¥:
        1. è¾“å…¥åˆ—éªŒè¯: æ£€æŸ¥ columns_to_extract ä¸­çš„åˆ—æ˜¯å¦å­˜åœ¨
        2. è¾“å‡ºåˆ—å‡†å¤‡: å¦‚æœ columns_to_write ä¸­çš„åˆ—ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
        
        æ³¨æ„:
            - è¾“å…¥åˆ—ä¸å­˜åœ¨åªå‘å‡ºè­¦å‘Šï¼Œä¸é˜»æ­¢è¿è¡Œ
            - è¾“å‡ºåˆ—ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼ˆå€¼ä¸º Noneï¼‰
        """
        column_names = self.engine.get_column_names(self.df)

        # æ£€æŸ¥è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨
        missing_extract = [c for c in self.columns_to_extract if c not in column_names]
        if missing_extract:
            logging.warning(f"è¾“å…¥åˆ— {missing_extract} åœ¨ Excel ä¸­ä¸å­˜åœ¨")

        # åˆ›å»ºè¾“å‡ºåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        for alias, out_col in self.columns_to_write.items():
            if not self.engine.has_column(self.df, out_col):
                logging.warning(f"è¾“å‡ºåˆ— '{out_col}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°åˆ—")
                self.df = self.engine.add_column(self.df, out_col, None)

    # ==================== æ ¸å¿ƒæ¥å£å®ç° ====================

    def get_total_task_count(self) -> int:
        """
        è·å–æœªå¤„ç†ä»»åŠ¡æ€»æ•°
        
        æ‰«ææ•´ä¸ª DataFrameï¼Œç»Ÿè®¡æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„è¡Œæ•°:
        1. è¾“å…¥åˆ—æ¡ä»¶æ»¡è¶³ï¼ˆæ ¹æ® require_all_input_fields é…ç½®ï¼‰
        2. ä»»ä¸€è¾“å‡ºåˆ—ä¸ºç©º
        
        Returns:
            int: æœªå¤„ç†ä»»åŠ¡æ•°é‡
            
        æ³¨æ„:
            æ­¤æ–¹æ³•ä¼šæ‰«æå…¨é‡æ•°æ®ï¼Œå¤§æ–‡ä»¶è€—æ—¶è¾ƒé•¿ã€‚
            å»ºè®®åœ¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ï¼Œè€Œéå¾ªç¯ä¸­è°ƒç”¨ã€‚
        """
        logging.info("æ­£åœ¨è®¡ç®— Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°...")

        min_idx, max_idx = self.engine.get_index_range(self.df)
        unprocessed = self._filter_unprocessed_indices(min_idx, max_idx)
        count = len(unprocessed)

        logging.info(f"Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°: {count}")
        return count

    def get_processed_task_count(self) -> int:
        """
        è·å–å·²å¤„ç†ä»»åŠ¡æ€»æ•°
        
        ç»Ÿè®¡æ‰€æœ‰è¾“å‡ºåˆ—éƒ½æœ‰éç©ºå€¼çš„è¡Œæ•°ã€‚
        ç”¨äºè¿›åº¦ç»Ÿè®¡å’Œ Token ä¼°ç®—é‡‡æ ·ã€‚
        
        Returns:
            int: å·²å¤„ç†ä»»åŠ¡æ•°é‡
            
        ç®—æ³•:
            éå†æ‰€æœ‰è¡Œï¼Œæ£€æŸ¥æ¯è¡Œçš„æ‰€æœ‰è¾“å‡ºåˆ—æ˜¯å¦éƒ½éç©ºã€‚
            ä½¿ç”¨å¼•æ“çš„ is_empty() æ–¹æ³•ç»Ÿä¸€åˆ¤æ–­ç©ºå€¼
            ï¼ˆåŒ…æ‹¬ Noneã€NaNã€ç©ºå­—ç¬¦ä¸²ç­‰ï¼‰ã€‚
        """
        logging.info("æ­£åœ¨è®¡ç®— Excel ä¸­å·²å¤„ç†çš„ä»»åŠ¡æ€»æ•°...")

        output_columns = list(self.columns_to_write.values())
        if not output_columns:
            return 0

        processed_count = 0
        with self.lock:
            all_indices = self.engine.get_indices(self.df)

            for idx in all_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    all_filled = True
                    for col in output_columns:
                        value = row_data.get(col)
                        # ä½¿ç”¨å¼•æ“çš„ is_empty æ–¹æ³•ç»Ÿä¸€å¤„ç†ç©ºå€¼åˆ¤æ–­
                        # åŒ…æ‹¬ Noneã€NaNã€ç©ºå­—ç¬¦ä¸²ç­‰å„ç§ç©ºå€¼ç±»å‹
                        if self.engine.is_empty(value):
                            all_filled = False
                            break

                    if all_filled:
                        processed_count += 1
                except Exception:
                    continue

        logging.info(f"Excel ä¸­å·²å¤„ç†çš„ä»»åŠ¡æ€»æ•°: {processed_count}")
        return processed_count

    def get_id_boundaries(self) -> tuple[int, int]:
        """
        è·å– DataFrame ç´¢å¼•è¾¹ç•Œ
        
        è¿”å› DataFrame çš„æœ€å°å’Œæœ€å¤§ç´¢å¼•å€¼ã€‚
        ç”¨äºåˆ†ç‰‡è°ƒåº¦å™¨åˆ’åˆ†å·¥ä½œåŒºé—´ã€‚
        
        Returns:
            tuple[int, int]: (æœ€å°ç´¢å¼•, æœ€å¤§ç´¢å¼•)
            å¦‚æœ DataFrame ä¸ºç©ºï¼Œè¿”å› (0, -1)
        
        æ³¨æ„:
            Excel/CSV ä½¿ç”¨è¡Œå·ä½œä¸ºç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰ã€‚
            å¦‚æœ DataFrame æœ‰è‡ªå®šä¹‰ç´¢å¼•ï¼Œä½¿ç”¨å¼•æ“æ–¹æ³•è·å–å®é™…èŒƒå›´ã€‚
        """
        if self.engine.row_count(self.df) == 0:
            return (0, -1)

        min_idx, max_idx = self.engine.get_index_range(self.df)
        logging.info(f"Excel DataFrame ç´¢å¼•èŒƒå›´: {min_idx} - {max_idx}")
        return (min_idx, max_idx)

    def initialize_shard(self, shard_id: int, min_idx: int, max_idx: int) -> int:
        """
        åˆå§‹åŒ–åˆ†ç‰‡ï¼ŒåŠ è½½æŒ‡å®šèŒƒå›´çš„æœªå¤„ç†ä»»åŠ¡
        
        åˆ†ç‰‡è°ƒåº¦çš„æ ¸å¿ƒæ–¹æ³•ã€‚å°†æŒ‡å®šç´¢å¼•èŒƒå›´å†…çš„æœªå¤„ç†ä»»åŠ¡
        åŠ è½½åˆ°å†…å­˜ä»»åŠ¡é˜Ÿåˆ—ä¸­ï¼Œä¾›åç»­ get_task_batch() è·å–ã€‚
        
        å·¥ä½œæµç¨‹:
            1. è¿‡æ»¤æœªå¤„ç†ç´¢å¼•ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
            2. æå–æ¯è¡Œçš„è¾“å…¥åˆ—æ•°æ®
            3. æ„å»º (ç´¢å¼•, æ•°æ®å­—å…¸) å…ƒç»„åˆ—è¡¨
            4. æ›´æ–°å†…å­˜ä»»åŠ¡é˜Ÿåˆ—

        Args:
            shard_id: åˆ†ç‰‡æ ‡è¯†ç¬¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            min_idx: åˆ†ç‰‡èµ·å§‹ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
            max_idx: åˆ†ç‰‡ç»“æŸç´¢å¼•ï¼ˆåŒ…å«ï¼‰

        Returns:
            int: å®é™…åŠ è½½çš„ä»»åŠ¡æ•°é‡
            
        æ³¨æ„:
            - ä½¿ç”¨å‘é‡åŒ–è¿‡æ»¤ï¼Œæ€§èƒ½æ¯”é€è¡Œéå†å¿« 50-100 å€
            - ä»»åŠ¡é˜Ÿåˆ—ä¼šè¢«å®Œå…¨æ›¿æ¢ï¼Œè€Œéè¿½åŠ 
            - åˆ†ç‰‡çŠ¶æ€ï¼ˆcurrent_shard_id ç­‰ï¼‰ä¼šè¢«æ›´æ–°
        """
        logging.info(f"å¼€å§‹åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx})...")

        shard_tasks: list[tuple[Any, dict[str, Any]]] = []

        try:
            # è¿‡æ»¤æœªå¤„ç†çš„ç´¢å¼•
            unprocessed_indices = self._filter_unprocessed_indices(min_idx, max_idx)

            if unprocessed_indices:
                logging.debug(
                    f"åˆ†ç‰‡ {shard_id}: æ‰¾åˆ° {len(unprocessed_indices)} ä¸ªæœªå¤„ç†ç´¢å¼•ï¼Œæ­£åœ¨æå–æ•°æ®..."
                )

                for idx in unprocessed_indices:
                    try:
                        row_data = self.engine.get_row(self.df, idx)
                        record_dict = {
                            col: self.engine.to_string(row_data.get(col, ""))
                            for col in self.columns_to_extract
                        }
                        shard_tasks.append((idx, record_dict))
                    except Exception as e:
                        logging.error(
                            f"åˆ†ç‰‡ {shard_id}: æå–ç´¢å¼• {idx} æ•°æ®æ—¶å‡ºé”™: {e}"
                        )
            else:
                logging.info(f"åˆ†ç‰‡ {shard_id}: åœ¨æŒ‡å®šç´¢å¼•èŒƒå›´å†…æœªæ‰¾åˆ°æœªå¤„ç†çš„ä»»åŠ¡")

        except Exception as e:
            logging.error(
                f"åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼• {min_idx}-{max_idx}) å¤±è´¥: {e}",
                exc_info=True,
            )
            shard_tasks = []

        # æ›´æ–°ä»»åŠ¡é˜Ÿåˆ—
        with self.lock:
            self.tasks = shard_tasks

        # æ›´æ–°åˆ†ç‰‡çŠ¶æ€
        self.current_shard_id = shard_id
        self.current_min_idx = min_idx
        self.current_max_idx = max_idx

        loaded_count = len(shard_tasks)
        logging.info(
            f"åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx}) åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½ä»»åŠ¡æ•°: {loaded_count}"
        )

        return loaded_count

    def get_task_batch(self, batch_size: int) -> list[tuple[Any, dict[str, Any]]]:
        """
        ä»å†…å­˜ä»»åŠ¡é˜Ÿåˆ—è·å–ä¸€æ‰¹ä»»åŠ¡
        
        ä»é˜Ÿåˆ—å¤´éƒ¨å¼¹å‡ºæŒ‡å®šæ•°é‡çš„ä»»åŠ¡ï¼Œç”¨äºå¹¶å‘å¤„ç†ã€‚
        
        Args:
            batch_size: è¯·æ±‚çš„ä»»åŠ¡æ•°é‡
            
        Returns:
            list[tuple[Any, dict[str, Any]]]: ä»»åŠ¡åˆ—è¡¨
                - å…ƒç»„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç´¢å¼•ï¼ˆç”¨äºç»“æœå†™å›ï¼‰
                - å…ƒç»„ç¬¬äºŒä¸ªå…ƒç´ æ˜¯è¾“å…¥æ•°æ®å­—å…¸
            å¦‚æœé˜Ÿåˆ—ä¸è¶³ï¼Œè¿”å›å‰©ä½™å…¨éƒ¨ä»»åŠ¡ã€‚
        
        çº¿ç¨‹å®‰å…¨:
            ä½¿ç”¨ self.lock ä¿æŠ¤é˜Ÿåˆ—æ“ä½œã€‚
        """
        with self.lock:
            batch = self.tasks[:batch_size]
            self.tasks = self.tasks[batch_size:]
            return batch

    def update_task_results(self, results: dict[int, dict[str, Any]]) -> None:
        """
        æ‰¹é‡å†™å›ä»»åŠ¡ç»“æœåˆ° DataFrame
        
        å°† AI å¤„ç†ç»“æœæ›´æ–°åˆ°å†…å­˜ DataFrame ä¸­ã€‚
        å¦‚æœè¾¾åˆ°ä¿å­˜é—´éš”ï¼Œè‡ªåŠ¨è§¦å‘æ–‡ä»¶ä¿å­˜ã€‚

        Args:
            results: ç»“æœå­—å…¸ {ç´¢å¼•: {åˆ«å: å€¼, ...}}
                ä¾‹: {0: {"result": "åˆ†æç»“æœ", "score": "0.95"}}
        
        å¤„ç†é€»è¾‘:
            1. è·³è¿‡åŒ…å« "_error" é”®çš„å¤±è´¥ç»“æœ
            2. æ ¹æ® columns_to_write æ˜ å°„å†™å…¥å¯¹åº”åˆ—
            3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ° save_intervalï¼Œè§¦å‘è‡ªåŠ¨ä¿å­˜
        
        è‡ªåŠ¨ä¿å­˜:
            - ä¿å­˜åœ¨é”å¤–æ‰§è¡Œï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
            - ä¿å­˜å¤±è´¥åªè®°å½•é”™è¯¯ï¼Œä¸æŠ›å‡ºå¼‚å¸¸
        
        æ³¨æ„:
            - ç»“æœä¸­çš„åˆ«åå¿…é¡»åœ¨ columns_to_write ä¸­å®šä¹‰
            - ç´¢å¼•å¿…é¡»å­˜åœ¨äº DataFrame ä¸­
        """
        if not results:
            return

        updated_indices: list[int] = []
        needs_save = False

        try:
            with self.lock:
                for idx, row_result in results.items():
                    # è·³è¿‡é”™è¯¯ç»“æœ
                    if "_error" in row_result:
                        continue

                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                    all_indices = self.engine.get_indices(self.df)
                    if idx not in all_indices:
                        logging.warning(f"å°è¯•æ›´æ–° Excel ä¸­ä¸å­˜åœ¨çš„ç´¢å¼• {idx}ï¼Œè·³è¿‡")
                        continue

                    # å†™å…¥ç»“æœ
                    for alias, col_name in self.columns_to_write.items():
                        if self.engine.has_column(self.df, col_name):
                            value = row_result.get(alias, "")
                            try:
                                self.df = self.engine.set_value(
                                    self.df, idx, col_name, value
                                )
                            except Exception as e:
                                logging.warning(
                                    f"è®¾ç½®ç´¢å¼• {idx} åˆ— '{col_name}' å€¼å¤±è´¥: {e}"
                                )

                    updated_indices.append(idx)

                if updated_indices:
                    logging.info(f"å·²åœ¨å†…å­˜ä¸­æ›´æ–° {len(updated_indices)} æ¡ Excel è®°å½•")

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜
                    current_time = time.time()
                    if current_time - self.last_save_time >= self.save_interval:
                        needs_save = True
                        self.last_save_time = current_time

        except Exception as e:
            logging.error(f"æ›´æ–° Excel DataFrame æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            needs_save = False

        # åœ¨é”å¤–æ‰§è¡Œä¿å­˜
        if needs_save:
            logging.info(
                f"è¾¾åˆ°ä¿å­˜é—´éš” ({self.save_interval}s)ï¼Œå‡†å¤‡ä¿å­˜ Excel æ–‡ä»¶..."
            )
            try:
                self._save_excel()
            except Exception as e:
                logging.error(f"è‡ªåŠ¨ä¿å­˜ Excel æ–‡ä»¶å¤±è´¥: {e}")

    def reload_task_data(self, idx: int) -> dict[str, Any] | None:
        """
        é‡æ–°åŠ è½½ä»»åŠ¡çš„åŸå§‹è¾“å…¥æ•°æ®
        
        ä» DataFrame ä¸­é‡æ–°è¯»å–æŒ‡å®šç´¢å¼•çš„è¾“å…¥åˆ—æ•°æ®ã€‚
        ç”¨äº API é”™è¯¯é‡è¯•æ—¶é‡æ–°è·å–åŸå§‹æ•°æ®ï¼Œé¿å…ä½¿ç”¨
        å¯èƒ½è¢«æ±¡æŸ“çš„ä»»åŠ¡å…ƒæ•°æ®ã€‚

        Args:
            idx: DataFrame ç´¢å¼•

        Returns:
            dict[str, Any] | None: è¾“å…¥æ•°æ®å­—å…¸ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨è¿”å› None
        
        ä½¿ç”¨åœºæ™¯:
            å½“ API è°ƒç”¨å¤±è´¥éœ€è¦é‡è¯•æ—¶ï¼ŒRetryStrategy ä¼šè°ƒç”¨æ­¤æ–¹æ³•
            é‡æ–°è·å–å¹²å‡€çš„è¾“å…¥æ•°æ®ï¼Œç¡®ä¿é‡è¯•ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ã€‚
        """
        try:
            with self.lock:
                all_indices = self.engine.get_indices(self.df)
                if idx not in all_indices:
                    logging.warning(
                        f"å°è¯•é‡è½½æ•°æ®å¤±è´¥: ç´¢å¼• {idx} åœ¨ DataFrame ä¸­ä¸å­˜åœ¨"
                    )
                    return None

                row_data = self.engine.get_row(self.df, idx)
                record_dict = {
                    col: self.engine.to_string(row_data.get(col, ""))
                    for col in self.columns_to_extract
                }
                return record_dict

        except Exception as e:
            logging.error(f"é‡è½½ç´¢å¼• {idx} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return None

    def close(self) -> None:
        """
        å…³é—­ä»»åŠ¡æ± å¹¶æ‰§è¡Œæœ€ç»ˆä¿å­˜
        
        åœ¨å¤„ç†ç»“æŸæ—¶è°ƒç”¨ï¼Œç¡®ä¿æ‰€æœ‰å†…å­˜ä¸­çš„æ•°æ®éƒ½è¢«æŒä¹…åŒ–ã€‚
        
        æ³¨æ„:
            - å³ä½¿ä¿å­˜å¤±è´¥ä¹Ÿä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼ˆå·²è®°å½•é”™è¯¯æ—¥å¿—ï¼‰
            - è°ƒç”¨åä¸åº”å†ä½¿ç”¨æ­¤ä»»åŠ¡æ± å®ä¾‹
        """
        logging.info("æ­£åœ¨æ‰§è¡Œ Excel æ–‡ä»¶çš„æœ€ç»ˆä¿å­˜æ“ä½œ...")
        try:
            self._save_excel()
        except Exception as e:
            logging.error(f"æœ€ç»ˆä¿å­˜ Excel æ–‡ä»¶å¤±è´¥: {e}")

    # ==================== å†…éƒ¨æ–¹æ³• ====================

    def _filter_unprocessed_indices(self, min_idx: int, max_idx: int) -> list[int]:
        """
        è¿‡æ»¤æŒ‡å®šèŒƒå›´å†…çš„æœªå¤„ç†ç´¢å¼•

        ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼Œæ€§èƒ½æ¯”é€è¡Œéå†å¿« 50-100 å€ã€‚
        """
        # ä½¿ç”¨å¼•æ“çš„å‘é‡åŒ–è¿‡æ»¤æ–¹æ³•
        output_columns = list(self.columns_to_write.values())

        # è·å–èŒƒå›´å†…çš„å­é›†
        sub_df = self.engine.slice_by_index_range(self.df, min_idx, max_idx)

        if self.engine.row_count(sub_df) == 0:
            return []

        # å‘é‡åŒ–è¿‡æ»¤
        try:
            unprocessed = self.engine.filter_indices_vectorized(
                sub_df,
                self.columns_to_extract,
                output_columns,
                self.require_all_input_fields,
                index_offset=min_idx,
            )
            logging.debug(
                f"è¿‡æ»¤ç´¢å¼•èŒƒå›´ {min_idx}-{max_idx} å®Œæˆï¼Œæ‰¾åˆ° {len(unprocessed)} ä¸ªæœªå¤„ç†ç´¢å¼•"
            )
            return unprocessed

        except Exception as e:
            logging.error(f"è¿‡æ»¤æœªå¤„ç†ç´¢å¼•æ—¶å‡ºé”™: {e}", exc_info=True)
            return []

    def _save_excel(self) -> None:
        """
        ä¿å­˜æ–‡ä»¶ï¼ˆExcel æˆ– CSVï¼‰

        æ ¹æ®æ–‡ä»¶ç±»å‹å’Œè¾“å‡ºè·¯å¾„è‡ªåŠ¨é€‰æ‹©ä¿å­˜æ–¹å¼ã€‚
        å¤„ç† Unicode ç¼–ç é—®é¢˜ï¼Œå¿…è¦æ—¶æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼æˆ–å›é€€åˆ° CSVã€‚
        """
        # æ£€æŸ¥è¾“å‡ºè·¯å¾„æ˜¯å¦ä¸º CSVï¼ˆå¯èƒ½è¾“å…¥è¾“å‡ºæ ¼å¼ä¸åŒï¼‰
        output_is_csv = self.output_path.suffix.lower() == ".csv"
        logging.info(f"æ­£åœ¨å°è¯•ä¿å­˜ DataFrame åˆ°: {self.output_path}")

        try:
            with self.lock:
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                output_dir = self.output_path.parent
                if output_dir and not output_dir.exists():
                    output_dir.mkdir(parents=True, exist_ok=True)

                # CSV æ–‡ä»¶ç›´æ¥ä¿å­˜
                if self._is_csv or output_is_csv:
                    self.engine.write_csv(self.df, self.output_path)
                    logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜åˆ°: {self.output_path}")
                    return

                # Excel æ–‡ä»¶ä¿å­˜ç­–ç•¥
                # ç­–ç•¥1: ç›´æ¥ä¿å­˜
                try:
                    self.engine.write_excel(self.df, self.output_path)
                    logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜åˆ°: {self.output_path}")
                    return

                except UnicodeEncodeError as e:
                    logging.error(f"âŒ Unicode ç¼–ç é—®é¢˜: {e}")
                    logging.info("ğŸ§¹ å¼€å§‹æ¸…ç©º AI è¾“å‡ºåˆ—ä¸­çš„é—®é¢˜å•å…ƒæ ¼...")

                    # ç­–ç•¥2: æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼
                    fixed_df = self.engine.copy(self.df)
                    fixed_df, cleared_count = self._clear_problematic_cells(fixed_df)

                    if cleared_count > 0:
                        logging.info(
                            f"ğŸ§¹ å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼ï¼Œé‡æ–°å°è¯•ä¿å­˜..."
                        )

                        try:
                            self.engine.write_excel(fixed_df, self.output_path)
                            logging.info(
                                f"âœ… DataFrame å·²æˆåŠŸä¿å­˜ (å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼)"
                            )
                            self.df = fixed_df
                            return
                        except UnicodeEncodeError:
                            logging.warning(
                                "âš ï¸ æ¸…ç©º AI è¾“å‡ºåˆ—åä»æœ‰é—®é¢˜ï¼Œå¯èƒ½æ¥è‡ªåŸå§‹æ•°æ®"
                            )

                    # ç­–ç•¥3: CSV å¤‡é€‰æ–¹æ¡ˆ
                    csv_path = self.output_path.with_suffix(".csv")
                    logging.warning(f"âš ï¸ Excel ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸º CSV: {csv_path}")

                    df_to_save = fixed_df if cleared_count > 0 else self.df
                    self.engine.write_csv(df_to_save, csv_path)
                    logging.warning(f"âœ… å·²ä¿å­˜ä¸º CSV: {csv_path}")

        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise IOError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}") from e

    def _clear_problematic_cells(self, df: Any) -> tuple[Any, int]:
        """
        æ¸…ç©º DataFrame ä¸­æœ‰ç¼–ç é—®é¢˜çš„å•å…ƒæ ¼

        åªæ£€æŸ¥ AI è¾“å‡ºåˆ—ï¼Œè¿”å›æ›´æ–°åçš„ DataFrame å’Œæ¸…ç©ºçš„å•å…ƒæ ¼æ•°é‡ã€‚
        """
        cleared_count = 0
        ai_columns = list(self.columns_to_write.values())
        updated_df = df

        for col_name in ai_columns:
            if not self.engine.has_column(df, col_name):
                continue

            for idx, row_data in self.engine.iter_rows(df, [col_name]):
                value = row_data.get(col_name)

                if isinstance(value, str) and value:
                    try:
                        value.encode("utf-8")
                    except UnicodeEncodeError:
                        logging.warning(
                            f"âŒ æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼: ç¬¬ {idx} è¡Œ, '{col_name}' åˆ—"
                        )
                        updated_df = self.engine.set_value(
                            updated_df, idx, col_name, ""
                        )
                        cleared_count += 1

        return updated_df, cleared_count

    # ==================== Token ä¼°ç®—é‡‡æ · ====================

    def sample_unprocessed_rows(self, sample_size: int) -> list[dict[str, Any]]:
        """
        é‡‡æ ·æœªå¤„ç†çš„è¡Œ (ç”¨äºè¾“å…¥ token ä¼°ç®—)

        Args:
            sample_size: é‡‡æ ·æ•°é‡

        Returns:
            é‡‡æ ·æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]
        """
        min_idx, max_idx = self.engine.get_index_range(self.df)
        unprocessed_indices = self._filter_unprocessed_indices(min_idx, max_idx)

        if not unprocessed_indices:
            return []

        # å–å‰ sample_size ä¸ª
        sample_indices = unprocessed_indices[:sample_size]
        samples = []

        with self.lock:
            for idx in sample_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in self.columns_to_extract
                    }
                    samples.append(record_dict)
                except Exception as e:
                    logging.warning(f"é‡‡æ ·ç´¢å¼• {idx} å¤±è´¥: {e}")

        logging.info(f"é‡‡æ · {len(samples)} æ¡æœªå¤„ç†è®°å½•ç”¨äºè¾“å…¥ token ä¼°ç®—")
        return samples

    def sample_processed_rows(self, sample_size: int) -> list[dict[str, Any]]:
        """
        é‡‡æ ·å·²å¤„ç†çš„è¡Œ (ç”¨äºè¾“å‡º token ä¼°ç®—)

        Args:
            sample_size: é‡‡æ ·æ•°é‡

        Returns:
            é‡‡æ ·æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]ï¼ŒåŒ…å«è¾“å‡ºåˆ—
        """
        output_columns = list(self.columns_to_write.values())

        # è¿‡æ»¤å·²å¤„ç†çš„è¡Œ (è¾“å‡ºåˆ—éƒ½éç©º)
        processed_indices = []

        with self.lock:
            all_indices = self.engine.get_indices(self.df)

            for idx in all_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)

                    # æ£€æŸ¥æ‰€æœ‰è¾“å‡ºåˆ—æ˜¯å¦éƒ½æœ‰å€¼
                    all_filled = True
                    for col in output_columns:
                        value = row_data.get(col)
                        if value is None or (
                            isinstance(value, str) and not value.strip()
                        ):
                            all_filled = False
                            break

                    if all_filled:
                        processed_indices.append(idx)
                        if len(processed_indices) >= sample_size:
                            break
                except Exception:
                    continue

        if not processed_indices:
            return []

        # æå–æ•°æ®
        samples = []
        with self.lock:
            for idx in processed_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    # åªæå–è¾“å‡ºåˆ—
                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in output_columns
                    }
                    samples.append(record_dict)
                except Exception as e:
                    logging.warning(f"é‡‡æ ·å·²å¤„ç†ç´¢å¼• {idx} å¤±è´¥: {e}")

        logging.info(f"é‡‡æ · {len(samples)} æ¡å·²å¤„ç†è®°å½•ç”¨äºè¾“å‡º token ä¼°ç®—")
        return samples

    def fetch_all_rows(self, columns: list[str]) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰è¡Œ (å¿½ç•¥å¤„ç†çŠ¶æ€)

        Args:
            columns: éœ€è¦æå–çš„åˆ—ååˆ—è¡¨

        Returns:
            æ‰€æœ‰è¡Œçš„æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]
        """
        all_rows = []

        with self.lock:
            all_indices = self.engine.get_indices(self.df)

            for idx in all_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in columns
                    }
                    all_rows.append(record_dict)
                except Exception as e:
                    logging.warning(f"è·å–ç´¢å¼• {idx} æ•°æ®å¤±è´¥: {e}")

        logging.info(f"å·²è·å– {len(all_rows)} æ¡è®°å½• (å¿½ç•¥å¤„ç†çŠ¶æ€)")
        return all_rows

    def fetch_all_processed_rows(self, columns: list[str]) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·²å¤„ç†è¡Œ (ä»…è¾“å‡ºå·²å®Œæˆçš„è®°å½•)

        Args:
            columns: éœ€è¦æå–çš„åˆ—ååˆ—è¡¨

        Returns:
            å·²å¤„ç†è¡Œçš„æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]
        """
        output_columns = list(self.columns_to_write.values())
        if not output_columns:
            return []

        processed_rows = []
        with self.lock:
            all_indices = self.engine.get_indices(self.df)

            for idx in all_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    all_filled = True
                    for col in output_columns:
                        value = row_data.get(col)
                        if value is None or (
                            isinstance(value, str) and not value.strip()
                        ):
                            all_filled = False
                            break

                    if not all_filled:
                        continue

                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in columns
                    }
                    processed_rows.append(record_dict)
                except Exception as e:
                    logging.warning(f"è·å–å·²å¤„ç†ç´¢å¼• {idx} æ•°æ®å¤±è´¥: {e}")

        logging.info(f"å·²è·å– {len(processed_rows)} æ¡å·²å¤„ç†è®°å½•")
        return processed_rows
