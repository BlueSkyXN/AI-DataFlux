"""
Excel æ•°æ®æºä»»åŠ¡æ± å®ç°

åŸºäº DataFrame å¼•æ“æŠ½è±¡ï¼Œæ”¯æŒ pandas å’Œ polars ç­‰å¤šç§å®ç°ã€‚
æ”¯æŒé«˜æ€§èƒ½è¯»å†™å™¨: calamine (fastexcel) å’Œ xlsxwriterã€‚
"""

import logging
import time
from pathlib import Path
from typing import Any

from .base import BaseTaskPool
from .engines import get_engine, BaseEngine


class ExcelTaskPool(BaseTaskPool):
    """
    Excel æ•°æ®æºä»»åŠ¡æ± 
    
    ä» Excel æ–‡ä»¶è¯»å–ä»»åŠ¡æ•°æ®ï¼Œå¤„ç†åå†™å›ç»“æœã€‚
    æ”¯æŒå®šæ—¶ä¿å­˜ã€åˆ†ç‰‡åŠ è½½ã€å‘é‡åŒ–è¿‡æ»¤ç­‰åŠŸèƒ½ã€‚
    
    Attributes:
        input_path: è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
        save_interval: è‡ªåŠ¨ä¿å­˜é—´éš” (ç§’)
        engine: DataFrame å¼•æ“å®ä¾‹
        df: å½“å‰ DataFrame (å¼•æ“ç‰¹å®šç±»å‹)
    """
    
    def __init__(
        self,
        input_path: str | Path,
        output_path: str | Path,
        columns_to_extract: list[str],
        columns_to_write: dict[str, str],
        save_interval: int = 300,
        require_all_input_fields: bool = True,
        engine_type: str = "pandas",
        excel_reader: str = "auto",
        excel_writer: str = "auto",
    ):
        """
        åˆå§‹åŒ– Excel ä»»åŠ¡æ± 
        
        Args:
            input_path: è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
            columns_to_extract: éœ€è¦æå–çš„åˆ—ååˆ—è¡¨
            columns_to_write: å†™å›æ˜ å°„ {åˆ«å: å®é™…åˆ—å}
            save_interval: è‡ªåŠ¨ä¿å­˜é—´éš” (ç§’)
            require_all_input_fields: æ˜¯å¦è¦æ±‚æ‰€æœ‰è¾“å…¥å­—æ®µéƒ½éç©º
            engine_type: DataFrame å¼•æ“ç±»å‹ ("pandas" | "polars" | "auto")
            excel_reader: Excel è¯»å–å™¨ ("openpyxl" | "calamine" | "auto")
            excel_writer: Excel å†™å…¥å™¨ ("openpyxl" | "xlsxwriter" | "auto")
        
        Raises:
            FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
            IOError: æ–‡ä»¶è¯»å–å¤±è´¥
        """
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        self.input_path = Path(input_path)
        self.output_path = Path(output_path)
        
        if not self.input_path.exists():
            raise FileNotFoundError(f"Excel è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_path}")
        
        # åˆå§‹åŒ–åŸºç±»
        super().__init__(columns_to_extract, columns_to_write, require_all_input_fields)
        
        # è·å– DataFrame å¼•æ“ (æ”¯æŒé«˜æ€§èƒ½è¯»å†™å™¨é…ç½®)
        self.engine: BaseEngine = get_engine(
            engine_type=engine_type,
            excel_reader=excel_reader,
            excel_writer=excel_writer,
        )
        logging.info(f"ä½¿ç”¨ DataFrame å¼•æ“: {self.engine.name}")
        
        # æ˜¾ç¤ºè¯»å†™å™¨ä¿¡æ¯
        if hasattr(self.engine, 'excel_reader'):
            logging.info(f"  - Excel è¯»å–å™¨: {self.engine.excel_reader}")
        if hasattr(self.engine, 'excel_writer'):
            logging.info(f"  - Excel å†™å…¥å™¨: {self.engine.excel_writer}")
        
        # è¯»å– Excel æ–‡ä»¶
        logging.info(f"æ­£åœ¨è¯»å– Excel æ–‡ä»¶: {self.input_path}")
        try:
            self.df = self.engine.read_excel(self.input_path)
            row_count = self.engine.row_count(self.df)
            logging.info(f"Excel æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {row_count} è¡Œ")
        except Exception as e:
            raise IOError(f"æ— æ³•è¯»å– Excel æ–‡ä»¶ {self.input_path}: {e}") from e
        
        # ä¿å­˜ç›¸å…³
        self.save_interval = save_interval
        self.last_save_time = time.time()
        
        # åˆ†ç‰‡çŠ¶æ€
        self.current_shard_id = -1
        self.current_min_idx = 0
        self.current_max_idx = 0
        
        # åˆ—éªŒè¯å’Œå‡†å¤‡
        self._validate_and_prepare_columns()
        
        logging.info(f"ExcelTaskPool åˆå§‹åŒ–å®Œæˆ | è¾“å…¥: {self.input_path}, è¾“å‡º: {self.output_path}")
    
    def _validate_and_prepare_columns(self) -> None:
        """éªŒè¯å’Œå‡†å¤‡åˆ—"""
        column_names = self.engine.get_column_names(self.df)
        
        # æ£€æŸ¥è¾“å…¥åˆ—
        missing_extract = [c for c in self.columns_to_extract if c not in column_names]
        if missing_extract:
            logging.warning(f"è¾“å…¥åˆ— {missing_extract} åœ¨ Excel ä¸­ä¸å­˜åœ¨")
        
        # åˆ›å»ºè¾“å‡ºåˆ— (å¦‚æœä¸å­˜åœ¨)
        for alias, out_col in self.columns_to_write.items():
            if not self.engine.has_column(self.df, out_col):
                logging.warning(f"è¾“å‡ºåˆ— '{out_col}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°åˆ—")
                self.df = self.engine.add_column(self.df, out_col, None)
    
    # ==================== æ ¸å¿ƒæ¥å£å®ç° ====================
    
    def get_total_task_count(self) -> int:
        """è·å–æœªå¤„ç†ä»»åŠ¡æ€»æ•°"""
        logging.info("æ­£åœ¨è®¡ç®— Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°...")
        
        min_idx, max_idx = self.engine.get_index_range(self.df)
        unprocessed = self._filter_unprocessed_indices(min_idx, max_idx)
        count = len(unprocessed)
        
        logging.info(f"Excel ä¸­æœªå¤„ç†çš„ä»»åŠ¡æ€»æ•°: {count}")
        return count
    
    def get_id_boundaries(self) -> tuple[int, int]:
        """è·å–ç´¢å¼•è¾¹ç•Œ"""
        if self.engine.row_count(self.df) == 0:
            return (0, -1)
        
        min_idx, max_idx = self.engine.get_index_range(self.df)
        logging.info(f"Excel DataFrame ç´¢å¼•èŒƒå›´: {min_idx} - {max_idx}")
        return (min_idx, max_idx)
    
    def initialize_shard(self, shard_id: int, min_idx: int, max_idx: int) -> int:
        """åˆå§‹åŒ–åˆ†ç‰‡ï¼ŒåŠ è½½æŒ‡å®šèŒƒå›´çš„æœªå¤„ç†ä»»åŠ¡"""
        logging.info(f"å¼€å§‹åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx})...")
        
        shard_tasks: list[tuple[Any, dict[str, Any]]] = []
        
        try:
            # è¿‡æ»¤æœªå¤„ç†çš„ç´¢å¼•
            unprocessed_indices = self._filter_unprocessed_indices(min_idx, max_idx)
            
            if unprocessed_indices:
                logging.debug(f"åˆ†ç‰‡ {shard_id}: æ‰¾åˆ° {len(unprocessed_indices)} ä¸ªæœªå¤„ç†ç´¢å¼•ï¼Œæ­£åœ¨æå–æ•°æ®...")
                
                for idx in unprocessed_indices:
                    try:
                        row_data = self.engine.get_row(self.df, idx)
                        record_dict = {
                            col: self.engine.to_string(row_data.get(col, ""))
                            for col in self.columns_to_extract
                        }
                        shard_tasks.append((idx, record_dict))
                    except Exception as e:
                        logging.error(f"åˆ†ç‰‡ {shard_id}: æå–ç´¢å¼• {idx} æ•°æ®æ—¶å‡ºé”™: {e}")
            else:
                logging.info(f"åˆ†ç‰‡ {shard_id}: åœ¨æŒ‡å®šç´¢å¼•èŒƒå›´å†…æœªæ‰¾åˆ°æœªå¤„ç†çš„ä»»åŠ¡")
                
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–åˆ†ç‰‡ {shard_id} (ç´¢å¼• {min_idx}-{max_idx}) å¤±è´¥: {e}", exc_info=True)
            shard_tasks = []
        
        # æ›´æ–°ä»»åŠ¡é˜Ÿåˆ—
        with self.lock:
            self.tasks = shard_tasks
        
        # æ›´æ–°åˆ†ç‰‡çŠ¶æ€
        self.current_shard_id = shard_id
        self.current_min_idx = min_idx
        self.current_max_idx = max_idx
        
        loaded_count = len(shard_tasks)
        logging.info(f"åˆ†ç‰‡ {shard_id} (ç´¢å¼•èŒƒå›´: {min_idx}-{max_idx}) åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½ä»»åŠ¡æ•°: {loaded_count}")
        
        return loaded_count
    
    def get_task_batch(self, batch_size: int) -> list[tuple[Any, dict[str, Any]]]:
        """ä»å†…å­˜é˜Ÿåˆ—è·å–ä¸€æ‰¹ä»»åŠ¡"""
        with self.lock:
            batch = self.tasks[:batch_size]
            self.tasks = self.tasks[batch_size:]
            return batch
    
    def update_task_results(self, results: dict[int, dict[str, Any]]) -> None:
        """æ‰¹é‡å†™å›ä»»åŠ¡ç»“æœ"""
        if not results:
            return
        
        updated_indices: list[int] = []
        needs_save = False
        
        try:
            with self.lock:
                for idx, row_result in results.items():
                    # è·³è¿‡é”™è¯¯ç»“æœ
                    if "_error" in row_result:
                        continue
                    
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                    all_indices = self.engine.get_indices(self.df)
                    if idx not in all_indices:
                        logging.warning(f"å°è¯•æ›´æ–° Excel ä¸­ä¸å­˜åœ¨çš„ç´¢å¼• {idx}ï¼Œè·³è¿‡")
                        continue
                    
                    # å†™å…¥ç»“æœ
                    for alias, col_name in self.columns_to_write.items():
                        if self.engine.has_column(self.df, col_name):
                            value = row_result.get(alias, "")
                            try:
                                self.df = self.engine.set_value(self.df, idx, col_name, value)
                            except Exception as e:
                                logging.warning(f"è®¾ç½®ç´¢å¼• {idx} åˆ— '{col_name}' å€¼å¤±è´¥: {e}")
                    
                    updated_indices.append(idx)
                
                if updated_indices:
                    logging.info(f"å·²åœ¨å†…å­˜ä¸­æ›´æ–° {len(updated_indices)} æ¡ Excel è®°å½•")
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜
                    current_time = time.time()
                    if current_time - self.last_save_time >= self.save_interval:
                        needs_save = True
                        self.last_save_time = current_time
                        
        except Exception as e:
            logging.error(f"æ›´æ–° Excel DataFrame æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            needs_save = False
        
        # åœ¨é”å¤–æ‰§è¡Œä¿å­˜
        if needs_save:
            logging.info(f"è¾¾åˆ°ä¿å­˜é—´éš” ({self.save_interval}s)ï¼Œå‡†å¤‡ä¿å­˜ Excel æ–‡ä»¶...")
            try:
                self._save_excel()
            except Exception as e:
                logging.error(f"è‡ªåŠ¨ä¿å­˜ Excel æ–‡ä»¶å¤±è´¥: {e}")
    
    def reload_task_data(self, idx: int) -> dict[str, Any] | None:
        """é‡æ–°åŠ è½½ä»»åŠ¡çš„åŸå§‹è¾“å…¥æ•°æ®"""
        try:
            with self.lock:
                all_indices = self.engine.get_indices(self.df)
                if idx not in all_indices:
                    logging.warning(f"å°è¯•é‡è½½æ•°æ®å¤±è´¥: ç´¢å¼• {idx} åœ¨ DataFrame ä¸­ä¸å­˜åœ¨")
                    return None
                
                row_data = self.engine.get_row(self.df, idx)
                record_dict = {
                    col: self.engine.to_string(row_data.get(col, ""))
                    for col in self.columns_to_extract
                }
                return record_dict
                
        except Exception as e:
            logging.error(f"é‡è½½ç´¢å¼• {idx} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return None
    
    def close(self) -> None:
        """å…³é—­å¹¶ä¿å­˜æ–‡ä»¶"""
        logging.info("æ­£åœ¨æ‰§è¡Œ Excel æ–‡ä»¶çš„æœ€ç»ˆä¿å­˜æ“ä½œ...")
        try:
            self._save_excel()
        except Exception as e:
            logging.error(f"æœ€ç»ˆä¿å­˜ Excel æ–‡ä»¶å¤±è´¥: {e}")
    
    # ==================== å†…éƒ¨æ–¹æ³• ====================
    
    def _filter_unprocessed_indices(self, min_idx: int, max_idx: int) -> list[int]:
        """
        è¿‡æ»¤æŒ‡å®šèŒƒå›´å†…çš„æœªå¤„ç†ç´¢å¼•
        
        ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼Œæ€§èƒ½æ¯”é€è¡Œéå†å¿« 50-100 å€ã€‚
        """
        # ä½¿ç”¨å¼•æ“çš„å‘é‡åŒ–è¿‡æ»¤æ–¹æ³•
        output_columns = list(self.columns_to_write.values())
        
        # è·å–èŒƒå›´å†…çš„å­é›†
        sub_df = self.engine.slice_by_index_range(self.df, min_idx, max_idx)
        
        if self.engine.row_count(sub_df) == 0:
            return []
        
        # å‘é‡åŒ–è¿‡æ»¤
        try:
            unprocessed = self.engine.filter_indices_vectorized(
                sub_df,
                self.columns_to_extract,
                output_columns,
                self.require_all_input_fields,
                index_offset=min_idx
            )
            logging.debug(f"è¿‡æ»¤ç´¢å¼•èŒƒå›´ {min_idx}-{max_idx} å®Œæˆï¼Œæ‰¾åˆ° {len(unprocessed)} ä¸ªæœªå¤„ç†ç´¢å¼•")
            return unprocessed
            
        except Exception as e:
            logging.error(f"è¿‡æ»¤æœªå¤„ç†ç´¢å¼•æ—¶å‡ºé”™: {e}", exc_info=True)
            return []
    
    def _save_excel(self) -> None:
        """
        ä¿å­˜ Excel æ–‡ä»¶
        
        å¤„ç† Unicode ç¼–ç é—®é¢˜ï¼Œå¿…è¦æ—¶æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼æˆ–å›é€€åˆ° CSVã€‚
        """
        logging.info(f"æ­£åœ¨å°è¯•ä¿å­˜ DataFrame åˆ°: {self.output_path}")
        
        try:
            with self.lock:
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                output_dir = self.output_path.parent
                if output_dir and not output_dir.exists():
                    output_dir.mkdir(parents=True, exist_ok=True)
                
                # ç­–ç•¥1: ç›´æ¥ä¿å­˜
                try:
                    self.engine.write_excel(self.df, self.output_path)
                    logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜åˆ°: {self.output_path}")
                    return
                    
                except UnicodeEncodeError as e:
                    logging.error(f"âŒ Unicode ç¼–ç é—®é¢˜: {e}")
                    logging.info("ğŸ§¹ å¼€å§‹æ¸…ç©º AI è¾“å‡ºåˆ—ä¸­çš„é—®é¢˜å•å…ƒæ ¼...")
                    
                    # ç­–ç•¥2: æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼
                    fixed_df = self.engine.copy(self.df)
                    fixed_df, cleared_count = self._clear_problematic_cells(fixed_df)
                    
                    if cleared_count > 0:
                        logging.info(f"ğŸ§¹ å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼ï¼Œé‡æ–°å°è¯•ä¿å­˜...")
                        
                        try:
                            self.engine.write_excel(fixed_df, self.output_path)
                            logging.info(f"âœ… DataFrame å·²æˆåŠŸä¿å­˜ (å·²æ¸…ç©º {cleared_count} ä¸ªé—®é¢˜å•å…ƒæ ¼)")
                            self.df = fixed_df
                            return
                        except UnicodeEncodeError:
                            logging.warning("âš ï¸ æ¸…ç©º AI è¾“å‡ºåˆ—åä»æœ‰é—®é¢˜ï¼Œå¯èƒ½æ¥è‡ªåŸå§‹æ•°æ®")
                    
                    # ç­–ç•¥3: CSV å¤‡é€‰æ–¹æ¡ˆ
                    csv_path = self.output_path.with_suffix(".csv")
                    logging.warning(f"âš ï¸ Excel ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸º CSV: {csv_path}")
                    
                    df_to_save = fixed_df if cleared_count > 0 else self.df
                    self.engine.write_csv(df_to_save, csv_path)
                    logging.warning(f"âœ… å·²ä¿å­˜ä¸º CSV: {csv_path}")
                    
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise IOError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}") from e
    
    def _clear_problematic_cells(self, df: Any) -> tuple[Any, int]:
        """
        æ¸…ç©º DataFrame ä¸­æœ‰ç¼–ç é—®é¢˜çš„å•å…ƒæ ¼
        
        åªæ£€æŸ¥ AI è¾“å‡ºåˆ—ï¼Œè¿”å›æ›´æ–°åçš„ DataFrame å’Œæ¸…ç©ºçš„å•å…ƒæ ¼æ•°é‡ã€‚
        """
        cleared_count = 0
        ai_columns = list(self.columns_to_write.values())
        updated_df = df
        
        for col_name in ai_columns:
            if not self.engine.has_column(df, col_name):
                continue
            
            for idx, row_data in self.engine.iter_rows(df, [col_name]):
                value = row_data.get(col_name)
                
                if isinstance(value, str) and value:
                    try:
                        value.encode("utf-8")
                    except UnicodeEncodeError:
                        logging.warning(f"âŒ æ¸…ç©ºé—®é¢˜å•å…ƒæ ¼: ç¬¬ {idx} è¡Œ, '{col_name}' åˆ—")
                        updated_df = self.engine.set_value(updated_df, idx, col_name, "")
                        cleared_count += 1
        
        return updated_df, cleared_count
    
    # ==================== Token ä¼°ç®—é‡‡æ · ====================
    
    def sample_unprocessed_rows(self, sample_size: int) -> list[dict[str, Any]]:
        """
        é‡‡æ ·æœªå¤„ç†çš„è¡Œ (ç”¨äºè¾“å…¥ token ä¼°ç®—)
        
        Args:
            sample_size: é‡‡æ ·æ•°é‡
            
        Returns:
            é‡‡æ ·æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]
        """
        min_idx, max_idx = self.engine.get_index_range(self.df)
        unprocessed_indices = self._filter_unprocessed_indices(min_idx, max_idx)
        
        if not unprocessed_indices:
            return []
        
        # å–å‰ sample_size ä¸ª
        sample_indices = unprocessed_indices[:sample_size]
        samples = []
        
        with self.lock:
            for idx in sample_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in self.columns_to_extract
                    }
                    samples.append(record_dict)
                except Exception as e:
                    logging.warning(f"é‡‡æ ·ç´¢å¼• {idx} å¤±è´¥: {e}")
        
        logging.info(f"é‡‡æ · {len(samples)} æ¡æœªå¤„ç†è®°å½•ç”¨äºè¾“å…¥ token ä¼°ç®—")
        return samples
    
    def sample_processed_rows(self, sample_size: int) -> list[dict[str, Any]]:
        """
        é‡‡æ ·å·²å¤„ç†çš„è¡Œ (ç”¨äºè¾“å‡º token ä¼°ç®—)
        
        Args:
            sample_size: é‡‡æ ·æ•°é‡
            
        Returns:
            é‡‡æ ·æ•°æ®åˆ—è¡¨ [{column: value, ...}, ...]ï¼ŒåŒ…å«è¾“å‡ºåˆ—
        """
        output_columns = list(self.columns_to_write.values())
        
        # è¿‡æ»¤å·²å¤„ç†çš„è¡Œ (è¾“å‡ºåˆ—éƒ½éç©º)
        processed_indices = []
        
        with self.lock:
            all_indices = self.engine.get_indices(self.df)
            
            for idx in all_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    
                    # æ£€æŸ¥æ‰€æœ‰è¾“å‡ºåˆ—æ˜¯å¦éƒ½æœ‰å€¼
                    all_filled = True
                    for col in output_columns:
                        value = row_data.get(col)
                        if value is None or (isinstance(value, str) and not value.strip()):
                            all_filled = False
                            break
                    
                    if all_filled:
                        processed_indices.append(idx)
                        if len(processed_indices) >= sample_size:
                            break
                except Exception:
                    continue
        
        if not processed_indices:
            return []
        
        # æå–æ•°æ®
        samples = []
        with self.lock:
            for idx in processed_indices:
                try:
                    row_data = self.engine.get_row(self.df, idx)
                    # åªæå–è¾“å‡ºåˆ—
                    record_dict = {
                        col: self.engine.to_string(row_data.get(col, ""))
                        for col in output_columns
                    }
                    samples.append(record_dict)
                except Exception as e:
                    logging.warning(f"é‡‡æ ·å·²å¤„ç†ç´¢å¼• {idx} å¤±è´¥: {e}")
        
        logging.info(f"é‡‡æ · {len(samples)} æ¡å·²å¤„ç†è®°å½•ç”¨äºè¾“å‡º token ä¼°ç®—")
        return samples
